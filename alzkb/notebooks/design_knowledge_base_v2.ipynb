{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AlzKB Design Notebook (v2)\n",
                "\n",
                "This notebook drives the design process for the Alzheimer's Knowledge Base (AlzKB).\n",
                "\n",
                "**Improvements over v1:**\n",
                "- Uses `MeetingContext` to manage phase-to-phase context dynamically\n",
                "- Auto-generates both narrative and structured (JSON) summaries\n",
                "- Persists discussions to disk (MD + JSON) automatically\n",
                "- No more hardcoded context strings between phases"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Imports complete.\n"
                    ]
                }
            ],
            "source": [
                "# Setup and Imports\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "\n",
                "# Ensure src is in pythonpath\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../src\")))\n",
                "\n",
                "from alzkb.meeting import run_meeting\n",
                "from alzkb.meeting_context import MeetingContext\n",
                "from alzkb.meeting_result import MeetingResult\n",
                "from alzkb.constants import (\n",
                "    PRINCIPAL_INVESTIGATOR, SCIENTIFIC_CRITIC, \n",
                "    KG_ENGINEER, ONTOLOGIST, VALIDATION_SCIENTIST,\n",
                "    MODEL_FLASH, MODEL_PRO, \n",
                "    BACKGROUND_PROMPT, TEAM_MEMBERS, CODE_GENERATION_RULES\n",
                ")\n",
                "from alzkb.agents import Agent\n",
                "\n",
                "print(\"Imports complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MeetingContext initialized.\n",
                        "Storage directory: ..\\discussions\n",
                        "Existing phases: []\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the MeetingContext\n",
                "# This will manage phase summaries and persist them to disk\n",
                "context = MeetingContext(\n",
                "    project_name=\"AlzKB\",\n",
                "    storage_dir=\"../discussions\"\n",
                ")\n",
                "\n",
                "print(f\"MeetingContext initialized.\")\n",
                "print(f\"Storage directory: {context.storage_dir}\")\n",
                "print(f\"Existing phases: {context.list_phases()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Team Selection\n",
                "\n",
                "**Objective**: Select 3 specialized agents to join the AlzKB implementation team.\n",
                "\n",
                "**Participants**: Principal Investigator (Lead) & Scientific Critic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the Agenda for Team Selection\n",
                "team_selection_agenda = f\"\"\"{BACKGROUND_PROMPT}\n",
                "TASK: Define 3 distinct Agents to form the AlzKB Implementation Team.\n",
                "\n",
                "PROCESS:\n",
                "1. PROPOSAL: The PI proposes 3 Agents with their specific system prompts.\n",
                "2. CRITIQUE: The Scientific Critic reviews the proposal for gaps, redundancy, or scientific validity.\n",
                "3. FINALIZATION: In the meeting summary, the PI MUST output the **Final Revised Python Code** for the 3 agents, incorporating the Critic's feedback.\n",
                "\n",
                "OUTPUT FORMAT: Python `Agent()` objects ONLY. No conversational filler for the code blocks.\n",
                "Each agent must have:\n",
                "- `title`: A descriptive role title.\n",
                "- `system_prompt`: A detailed persona description including roles and responsibilities.\n",
                "\n",
                "Do not include yourself (PI or Critic). \n",
                "Select roles that cover key technical and scientific needs (e.g., Knowledge Graph Engineering, Ontology, Data Science).\n",
                "\"\"\"\n",
                "\n",
                "print(\"Agenda defined.\")\n",
                "print(\"=\"*50)\n",
                "print(team_selection_agenda)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the Meeting\n",
                "print(\"Starting Team Selection Meeting...\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "result_team_selection = run_meeting(\n",
                "    meeting_type=\"individual\",\n",
                "    agenda=team_selection_agenda,\n",
                "    topic=\"Team Selection\",\n",
                "    team_member=PRINCIPAL_INVESTIGATOR,\n",
                "    num_rounds=1,\n",
                "    model_name=MODEL_FLASH\n",
                ")\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"Meeting Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store the result in the context (auto-saves to disk as MD + JSON)\n",
                "context.add_result(\"team_selection\", result_team_selection)\n",
                "\n",
                "print(f\"Phase 'team_selection' saved.\")\n",
                "print(f\"Storage location: {context.storage_dir / 'team_selection'}\")\n",
                "print(f\"Phases in context: {context.list_phases()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the structured summary\n",
                "print(\"=\" * 50)\n",
                "print(\"STRUCTURED SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "print(json.dumps(result_team_selection.summary_structured, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the narrative summary\n",
                "print(\"=\" * 50)\n",
                "print(\"NARRATIVE SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(result_team_selection.summary_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview the context that will be passed to the next phase\n",
                "print(\"=\" * 50)\n",
                "print(\"CONTEXT FOR NEXT PHASE\")\n",
                "print(\"=\" * 50)\n",
                "print(context.get_previous_context([\"team_selection\"]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Building Plan\n",
                "\n",
                "**Objective**: Develop a concrete building plan for the AlzKB knowledge graph.\n",
                "\n",
                "**Participants**: The full team (PI, KG Engineer, Ontologist, Validation Scientist, Scientific Critic).\n",
                "\n",
                "**Focus Areas:**\n",
                "1. **Ontology Creation**: Use domain knowledge to design the schema\n",
                "2. **Data Collection**: Identify and collect data sources to populate the ontology\n",
                "3. **Graph Database**: Convert the ontology into a graph database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get context from previous phase\n",
                "prev_context = context.get_previous_context([\"team_selection\"])\n",
                "\n",
                "# Define the Agenda for Building Plan\n",
                "building_plan_agenda = f\"\"\"{BACKGROUND_PROMPT}\n",
                "{prev_context}\n",
                "TASK: Develop a concrete, step-by-step Building Plan for the AlzKB Knowledge Graph.\n",
                "\n",
                "The plan MUST address these three phases in order:\n",
                "\n",
                "PHASE A - ONTOLOGY CREATION:\n",
                "- Use domain knowledge of Alzheimer's Disease to define the core ontology schema\n",
                "- Define key entities (e.g., Genes, Proteins, Diseases, Biomarkers, Pathways)\n",
                "- Define relationships between entities\n",
                "- Align with existing ontologies (e.g., SNOMED CT, Gene Ontology, Disease Ontology)\n",
                "\n",
                "PHASE B - DATA COLLECTION & POPULATION:\n",
                "- Identify authoritative data sources (e.g., ADNI, AMP-AD, UniProt, dbSNP)\n",
                "- Define data extraction strategies for each source\n",
                "- Plan for data normalization and entity resolution\n",
                "- Define quality criteria and validation rules\n",
                "\n",
                "PHASE C - GRAPH DATABASE IMPLEMENTATION:\n",
                "- Choose the appropriate graph database technology (e.g., Neo4j, GraphDB, Amazon Neptune)\n",
                "- Convert the ontology into a graph schema\n",
                "- Define indexing and query optimization strategies\n",
                "- Plan for RAG (Retrieval-Augmented Generation) integration\n",
                "\n",
                "Each team member should contribute their expertise:\n",
                "- Ontologist: Schema design and ontology alignment\n",
                "- KG Engineer: Data pipelines and database implementation\n",
                "- Validation Scientist: Quality assurance and RAG optimization\n",
                "- Scientific Critic: Challenge assumptions and identify risks\n",
                "\n",
                "OUTPUT GOAL: A phased roadmap with specific deliverables for each phase.\n",
                "\"\"\"\n",
                "\n",
                "print(\"Building Plan Agenda defined.\")\n",
                "print(\"=\" * 50)\n",
                "print(building_plan_agenda)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the Team Meeting\n",
                "print(\"Starting Building Plan Team Meeting...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "result_building_plan = run_meeting(\n",
                "    meeting_type=\"team\",\n",
                "    agenda=building_plan_agenda,\n",
                "    topic=\"AlzKB Building Plan\",\n",
                "    team_lead=PRINCIPAL_INVESTIGATOR,\n",
                "    team_members=TEAM_MEMBERS,\n",
                "    num_rounds=2,  # 2 rounds for thorough discussion\n",
                "    model_name=MODEL_PRO  # Use Pro model for complex planning\n",
                ")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Meeting Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store the result in the context\n",
                "context.add_result(\"building_plan\", result_building_plan)\n",
                "\n",
                "print(f\"Phase 'building_plan' saved.\")\n",
                "print(f\"Storage location: {context.storage_dir / 'building_plan'}\")\n",
                "print(f\"Phases in context: {context.list_phases()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the structured summary\n",
                "print(\"=\" * 50)\n",
                "print(\"BUILDING PLAN - STRUCTURED SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "print(json.dumps(result_building_plan.summary_structured, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the narrative summary\n",
                "print(\"=\" * 50)\n",
                "print(\"BUILDING PLAN - NARRATIVE SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(result_building_plan.summary_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview the accumulated context for future phases\n",
                "print(\"=\" * 50)\n",
                "print(\"ACCUMULATED CONTEXT (Team Selection + Building Plan)\")\n",
                "print(\"=\" * 50)\n",
                "print(context.get_previous_context([\"team_selection\", \"building_plan\"]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Ontology Creation\n",
                "\n",
                "**Objective**: Generate the final OWL ontology file for AlzKB.\n",
                "\n",
                "**Participants**: Ontologist (Lead) & Scientific Critic.\n",
                "\n",
                "**Deliverable**: A complete OWL/Turtle file including:\n",
                "- Haplotype-aware backbone (Patient → Genotype → Allele → Gene)\n",
                "- Cognitive Resilience class with numerical cutoffs\n",
                "- Proven vs. Hypothesized mechanism edge types\n",
                "- SHACL shapes for validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get context from previous phases\n",
                "prev_context = context.get_previous_context([\"team_selection\", \"building_plan\"])\n",
                "\n",
                "# Define the Agenda for Ontology Creation\n",
                "ontology_agenda = f\"\"\"{BACKGROUND_PROMPT}\n",
                "{prev_context}\n",
                "\n",
                "TASK: Generate the FINAL OWL ontology file for AlzKB.\n",
                "\n",
                "Based on the Building Plan decisions, you MUST create a complete ontology that includes:\n",
                "\n",
                "1. HAPLOTYPE-AWARE BACKBONE:\n",
                "   - Graph Pattern: (:Patient)-[:HAS_GENOTYPE]->(:Genotype)-[:COMPOSED_OF]->(:Allele)-[:VARIANT_OF]->(:Gene)\n",
                "   - Support for zygosity (Homozygous/Heterozygous)\n",
                "   - APOE allele representation (e2, e3, e4)\n",
                "\n",
                "2. COGNITIVE RESILIENCE CLASS (alzkb:Cognitive_Resilience):\n",
                "   - OWL Intersection Of:\n",
                "     * has_amyloid_status value Positive\n",
                "     * has_tau_status value Positive  \n",
                "     * has_cdr_global_score value 0.0\n",
                "     * has_mmse_score >= 29\n",
                "\n",
                "3. LOGIC LAYER - PROVEN vs HYPOTHESIZED:\n",
                "   - Edge Type 1: cl:has_mechanism_of_action (STRICT - FDA/Phase III only)\n",
                "   - Edge Type 2: alzkb:hypothesized_mechanism (INFERRED - must carry inference_confidence: 'LOW')\n",
                "\n",
                "4. CORE CLASSES:\n",
                "   - Gene, Protein, Disease, Biomarker, Pathway\n",
                "   - Patient, ClinicalObservation, GeneticVariant\n",
                "   - DrugTarget, TherapeuticIntervention\n",
                "\n",
                "5. STANDARD ALIGNMENTS:\n",
                "   - SNOMED CT for clinical terms\n",
                "   - Gene Ontology (GO) for biological processes\n",
                "   - Disease Ontology (DOID) for diseases\n",
                "   - UniProt for proteins\n",
                "\n",
                "{CODE_GENERATION_RULES}\n",
                "\n",
                "OUTPUT: Complete OWL ontology in Turtle (.ttl) syntax.\n",
                "The file should be production-ready and syntactically valid.\n",
                "\"\"\"\n",
                "\n",
                "print(\"Ontology Creation Agenda defined.\")\n",
                "print(\"=\" * 50)\n",
                "print(ontology_agenda)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the Individual Meeting with Ontologist\n",
                "print(\"Starting Ontology Creation Meeting...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "result_ontology = run_meeting(\n",
                "    meeting_type=\"individual\",\n",
                "    agenda=ontology_agenda,\n",
                "    topic=\"Ontology Creation\",\n",
                "    team_member=ONTOLOGIST,\n",
                "    num_rounds=1,\n",
                "    model_name=MODEL_PRO  # Use Pro for code generation\n",
                ")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Meeting Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store the result in the context\n",
                "context.add_result(\"ontology_creation\", result_ontology)\n",
                "\n",
                "print(f\"Phase 'ontology_creation' saved.\")\n",
                "print(f\"Storage location: {context.storage_dir / 'ontology_creation'}\")\n",
                "print(f\"Phases in context: {context.list_phases()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the narrative summary (should contain the OWL file)\n",
                "print(\"=\" * 50)\n",
                "print(\"ONTOLOGY CREATION - NARRATIVE SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(result_ontology.summary_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the full discussion history to extract the OWL file\n",
                "print(\"=\" * 50)\n",
                "print(\"FULL DISCUSSION (for OWL file extraction)\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "history = result_ontology.get_history()\n",
                "for turn in history:\n",
                "    role = turn.role\n",
                "    text = turn.parts[0].text if turn.parts else \"[No Content]\"\n",
                "    print(f\"\\n### {role}\\n\")\n",
                "    print(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the structured summary\n",
                "print(\"=\" * 50)\n",
                "print(\"ONTOLOGY CREATION - STRUCTURED SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "print(json.dumps(result_ontology.summary_structured, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Data Resources Selection\n",
                "\n",
                "**Objective**: Identify and evaluate data sources to populate the AlzKB ontology.\n",
                "\n",
                "**Participants**: The full team (PI, KG Engineer, Ontologist, Validation Scientist, Scientific Critic).\n",
                "\n",
                "**Focus Areas:**\n",
                "- Authoritative Alzheimer's research databases\n",
                "- Genomic and proteomic resources\n",
                "- Clinical trial and drug databases\n",
                "- Quality and accessibility criteria"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get context from previous phases\n",
                "prev_context = context.get_previous_context([\"building_plan\", \"ontology_creation\"])\n",
                "\n",
                "# Define the Agenda for Data Resources Selection\n",
                "data_resources_agenda = f\"\"\"{BACKGROUND_PROMPT}\n",
                "{prev_context}\n",
                "\n",
                "TASK: Select and evaluate DATA RESOURCES to populate the AlzKB knowledge graph.\n",
                "\n",
                "The ontology is now complete (alzkb-ontology-v2.ttl). We need REAL DATA to populate it.\n",
                "\n",
                "For each data source, the team must evaluate:\n",
                "1. RELEVANCE: Does it contain entities/relationships defined in our ontology?\n",
                "2. QUALITY: Is it authoritative, peer-reviewed, regularly updated?\n",
                "3. ACCESSIBILITY: Is it publicly available? API access? Licensing?\n",
                "4. FORMAT: What format (CSV, JSON, RDF, SPARQL endpoint)? ETL complexity?\n",
                "\n",
                "CATEGORIES TO COVER:\n",
                "\n",
                "A. PATIENT & CLINICAL DATA:\n",
                "   - Longitudinal cohort studies (e.g., ADNI, NACC, UK Biobank)\n",
                "   - Biomarker measurements (CSF, PET imaging)\n",
                "   - Cognitive assessments (MMSE, CDR scores)\n",
                "\n",
                "B. GENOMIC & GENETIC DATA:\n",
                "   - GWAS catalogs (NHGRI-EBI GWAS Catalog)\n",
                "   - Variant databases (dbSNP, ClinVar)\n",
                "   - Gene-disease associations (DisGeNET, OMIM)\n",
                "\n",
                "C. MOLECULAR & PATHWAY DATA:\n",
                "   - Protein databases (UniProt, STRING)\n",
                "   - Pathway databases (KEGG, Reactome)\n",
                "   - Gene Ontology annotations\n",
                "\n",
                "D. DRUG & THERAPEUTIC DATA:\n",
                "   - Drug databases (DrugBank, ChEMBL)\n",
                "   - Clinical trials (ClinicalTrials.gov)\n",
                "   - FDA drug labels\n",
                "\n",
                "Each team member should contribute:\n",
                "- KG Engineer: Assess ETL complexity and data formats\n",
                "- Ontologist: Map data fields to ontology classes/properties\n",
                "- Validation Scientist: Define quality gates for each source\n",
                "- Scientific Critic: Identify gaps and potential biases\n",
                "\n",
                "OUTPUT GOAL: A prioritized list of data sources with:\n",
                "- Source name and URL\n",
                "- Key entities it provides\n",
                "- Priority tier (Tier 1: Must Have, Tier 2: Should Have, Tier 3: Nice to Have)\n",
                "- Ingestion approach\n",
                "\"\"\"\n",
                "\n",
                "print(\"Data Resources Agenda defined.\")\n",
                "print(\"=\" * 50)\n",
                "print(data_resources_agenda)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the Team Meeting\n",
                "print(\"Starting Data Resources Selection Meeting...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "result_data_resources = run_meeting(\n",
                "    meeting_type=\"team\",\n",
                "    agenda=data_resources_agenda,\n",
                "    topic=\"Data Resources Selection\",\n",
                "    team_lead=PRINCIPAL_INVESTIGATOR,\n",
                "    team_members=TEAM_MEMBERS,\n",
                "    num_rounds=2,  # 2 rounds for comprehensive evaluation\n",
                "    model_name=MODEL_PRO\n",
                ")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Meeting Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store the result in the context\n",
                "context.add_result(\"data_resources\", result_data_resources)\n",
                "\n",
                "print(f\"Phase 'data_resources' saved.\")\n",
                "print(f\"Storage location: {context.storage_dir / 'data_resources'}\")\n",
                "print(f\"Phases in context: {context.list_phases()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the structured summary\n",
                "print(\"=\" * 50)\n",
                "print(\"DATA RESOURCES - STRUCTURED SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "print(json.dumps(result_data_resources.summary_structured, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the narrative summary\n",
                "print(\"=\" * 50)\n",
                "print(\"DATA RESOURCES - NARRATIVE SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(result_data_resources.summary_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview all accumulated context\n",
                "print(\"=\" * 50)\n",
                "print(\"ALL PHASES CONTEXT\")\n",
                "print(\"=\" * 50)\n",
                "print(context.get_previous_context())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Data Download Scripts\n",
                "\n",
                "**Objective**: Write Python scripts to download publicly accessible data resources.\n",
                "\n",
                "**Participants**: KG Engineer (Lead) & Scientific Critic.\n",
                "\n",
                "**Focus**: Public API/downloadable data sources only:\n",
                "- NHGRI-EBI GWAS Catalog\n",
                "- Reactome (BioPAX)\n",
                "- Ensembl/dbSNP (REST API)\n",
                "- DisGeNET (Curated)\n",
                "- ChEMBL (Binding assays)\n",
                "\n",
                "**Excluded**: ADNI and other resources requiring manual download/registration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data Download Scripts Agenda defined.\n",
                        "==================================================\n",
                        "Task: Build a scalable, retrieval-optimized Knowledge Graph for Alzheimer's Disease research.\n",
                        "--- PREVIOUS PHASES CONTEXT ---\n",
                        "\n",
                        "### DATA RESOURCES\n",
                        "The project has finalized the Data Resource Selection phase and authorized the Ingestion of Tier 1 sources (ADNI, GWAS Catalog, Reactome). The architecture now strictly handles demographic bias and clinical ambiguity, with validation protocols calibrated to preserve the 'Cognitive Resilience' patient signal.\n",
                        "\n",
                        "**Key Decisions:**\n",
                        "  1. Prioritize ADNI 'Master' files (ADNIMERGE, UPENNBIOMK_MASTER) over raw assays to guarantee biomarker harmonization.\n",
                        "  2. Enforce mandatory Ancestry Tagging (mapped to NCIT) on all GWAS associations to prevent Euro-centric RAG bias.\n",
                        "  3. Define APOE e2 alleles as explicit 'ResilienceFactor' nodes, logically disjoint from e4 'RiskIncrease' nodes.\n",
                        "  4. Implement strict quality gates: GWAS (p < 5e-8, N > 10k), DisGeNET (Human only, Score > 0.5), Reactome (Human only).\n",
                        "  5. Adopt a 'Trinary' Biomarker logic (Positive/Negative/Equivocal) to prevent forced classification of indeterminate clinical data.\n",
                        "\n",
                        "**Action Items:**\n",
                        "  - Deploy 'etl_adni_harmonized.py' using the approved Master files and fallback logic (KG Engineer)\n",
                        "  - Commit 'alzkb-ontology-v2.1.ttl' incorporating PopulationContext and Resilience schema updates (Ontologist)\n",
                        "  - Implement NLP parser for GWAS 'Initial Sample Description' to generate ancestry tags (KG Engineer)\n",
                        "  - Add textual description properties to Reactome and DisGeNET nodes for vector embedding (KG Engineer)\n",
                        "  - Execute the 'Resilience Stress Test' (verifying presence of A+T+N+/CDR 0 cohort) post-ingestion (Validation Scientist)\n",
                        "\n",
                        "**Status:** COMPLETE\n",
                        "\n",
                        "--- END PREVIOUS CONTEXT ---\n",
                        "\n",
                        "\n",
                        "TASK: Write Python scripts to DOWNLOAD the approved public data resources.\n",
                        "\n",
                        "FOCUS ON THESE PUBLICLY ACCESSIBLE SOURCES ONLY:\n",
                        "\n",
                        "1. NHGRI-EBI GWAS CATALOG:\n",
                        "   - URL: https://www.ebi.ac.uk/gwas/api/search/downloads/full\n",
                        "   - Format: TSV download\n",
                        "   - Filter: Alzheimer's related associations\n",
                        "\n",
                        "2. REACTOME:\n",
                        "   - URL: https://reactome.org/download-data\n",
                        "   - Format: BioPAX or pathway downloads\n",
                        "   - Filter: Human pathways only\n",
                        "\n",
                        "3. ENSEMBL / dbSNP:\n",
                        "   - Ensembl REST API: https://rest.ensembl.org\n",
                        "   - For variant resolution (RSIDs)\n",
                        "\n",
                        "4. DisGeNET:\n",
                        "   - URL: https://www.disgenet.org/downloads\n",
                        "   - Format: TSV/CSV curated associations\n",
                        "   - Filter: Alzheimer's disease (DOID:10652)\n",
                        "\n",
                        "5. ChEMBL:\n",
                        "   - REST API: https://www.ebi.ac.uk/chembl/api/data\n",
                        "   - For drug-target binding data\n",
                        "\n",
                        "EXCLUDED (require manual download):\n",
                        "- ADNI (requires application and registration)\n",
                        "- UK Biobank (requires application)\n",
                        "- Any other gated resources\n",
                        "\n",
                        "FOR EACH DATA SOURCE, PROVIDE:\n",
                        "1. A complete, runnable Python script\n",
                        "2. Proper error handling and logging\n",
                        "3. Progress indicators for large downloads\n",
                        "4. Output saved to ../data/raw/<source_name>/ directory\n",
                        "5. A README comment explaining the data structure\n",
                        "\n",
                        "\n",
                        "CRITICAL CODE GENERATION RULES:\n",
                        "1. OUTPUT FORMAT: Provide ONLY the code block (e.g., Python, Turtle, SPARQL). Do not wrap it in markdown triple backticks if possible, or if you do, ensure it is clean.\n",
                        "2. NO FILLER: Do not include \"Here is the code\" or \"I have updated the file\". Just the code.\n",
                        "3. COMPLETENESS: The code must be fully functional and complete. No placeholders like `# ... logic here`.\n",
                        "4. STANDARDS: \n",
                        "   - Python: PEP 8, typed, docstrings.\n",
                        "   - Ontology: Turtle format (`.ttl`), valid OWL/SHACL.\n",
                        "   - Database: Valid SPARQL or Cypher.\n",
                        "\n",
                        "\n",
                        "OUTPUT: Complete Python scripts for each data source.\n",
                        "Scripts should be production-ready and can be run independently.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Get context from data resources phase\n",
                "prev_context = context.get_previous_context([\"data_resources\"])\n",
                "\n",
                "# Define the Agenda for Data Download Scripts\n",
                "download_scripts_agenda = f\"\"\"{BACKGROUND_PROMPT}\n",
                "{prev_context}\n",
                "\n",
                "TASK: Write Python scripts to DOWNLOAD the approved public data resources.\n",
                "\n",
                "FOCUS ON THESE PUBLICLY ACCESSIBLE SOURCES ONLY:\n",
                "\n",
                "1. NHGRI-EBI GWAS CATALOG:\n",
                "   - URL: https://www.ebi.ac.uk/gwas/api/search/downloads/full\n",
                "   - Format: TSV download\n",
                "   - Filter: Alzheimer's related associations\n",
                "\n",
                "2. REACTOME:\n",
                "   - URL: https://reactome.org/download-data\n",
                "   - Format: BioPAX or pathway downloads\n",
                "   - Filter: Human pathways only\n",
                "\n",
                "3. ENSEMBL / dbSNP:\n",
                "   - Ensembl REST API: https://rest.ensembl.org\n",
                "   - For variant resolution (RSIDs)\n",
                "\n",
                "4. DisGeNET:\n",
                "   - URL: https://www.disgenet.org/downloads\n",
                "   - Format: TSV/CSV curated associations\n",
                "   - Filter: Alzheimer's disease (DOID:10652)\n",
                "\n",
                "5. ChEMBL:\n",
                "   - REST API: https://www.ebi.ac.uk/chembl/api/data\n",
                "   - For drug-target binding data\n",
                "\n",
                "EXCLUDED (require manual download):\n",
                "- ADNI (requires application and registration)\n",
                "- UK Biobank (requires application)\n",
                "- Any other gated resources\n",
                "\n",
                "FOR EACH DATA SOURCE, PROVIDE:\n",
                "1. A complete, runnable Python script\n",
                "2. Proper error handling and logging\n",
                "3. Progress indicators for large downloads\n",
                "4. Output saved to ../data/raw/<source_name>/ directory\n",
                "5. A README comment explaining the data structure\n",
                "\n",
                "{CODE_GENERATION_RULES}\n",
                "\n",
                "OUTPUT: Complete Python scripts for each data source.\n",
                "Scripts should be production-ready and can be run independently.\n",
                "\"\"\"\n",
                "\n",
                "print(\"Data Download Scripts Agenda defined.\")\n",
                "print(\"=\" * 50)\n",
                "print(download_scripts_agenda)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Data Download Scripts Meeting...\n",
                        "==================================================\n",
                        "Starting individual meeting on: Data Download Scripts\n",
                        "\n",
                        "--- Round 1/1 ---\n",
                        "\n",
                        ">> Data Ingestion & Quality Engineer:\n",
                        "```python\n",
                        "#!/usr/bin/env python3\n",
                        "\"\"\"\n",
                        "ALZKB DATA INGESTION PIPELINE - TIER 1 RAW DATA DOWNLOADER\n",
                        "----------------------------------------------------------\n",
                        "Author: Lead Data Engineer, AlzKB\n",
                        "Version: 1....\n",
                        "\n",
                        ">> Scientific Critic (AlzKB):\n",
                        "Reviewing code submission.\n",
                        "\n",
                        "**STATUS: REVISION REQUIRED**\n",
                        "\n",
                        "The provided script fails to strictly adhere to the quality gates and scientific scope defined in the \"Previous Phases Context.\" While the co...\n",
                        "\n",
                        "--- Generating Summaries ---\n",
                        "\n",
                        ">> SUMMARY:\n",
                        "### MEETING SUMMARY: Data Ingestion Protocol (Tier 1 Public Sources)\n",
                        "\n",
                        "**Date:** October 27, 2023\n",
                        "**Attendees:** Lead Data Engineer, Scientific Critic, Ontologist, Validation Scientist\n",
                        "\n",
                        "**Outcome:**\n",
                        "The initial data ingestion strategy has been **REVISED** following critical review. The Scientific Cri...\n",
                        "==================================================\n",
                        "Meeting Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Run the Individual Meeting with KG Engineer\n",
                "print(\"Starting Data Download Scripts Meeting...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "result_download_scripts = run_meeting(\n",
                "    meeting_type=\"individual\",\n",
                "    agenda=download_scripts_agenda,\n",
                "    topic=\"Data Download Scripts\",\n",
                "    team_member=KG_ENGINEER,\n",
                "    num_rounds=1,\n",
                "    model_name=MODEL_PRO  # Use Pro for code generation\n",
                ")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Meeting Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Phase 'download_scripts' saved.\n",
                        "Storage location: ..\\discussions\\download_scripts\n",
                        "Phases in context: ['team_selection', 'building_plan', 'ontology_creation', 'data_resources', 'download_scripts']\n"
                    ]
                }
            ],
            "source": [
                "# Store the result in the context\n",
                "context.add_result(\"download_scripts\", result_download_scripts)\n",
                "\n",
                "print(f\"Phase 'download_scripts' saved.\")\n",
                "print(f\"Storage location: {context.storage_dir / 'download_scripts'}\")\n",
                "print(f\"Phases in context: {context.list_phases()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the full discussion (contains the scripts)\n",
                "print(\"=\" * 50)\n",
                "print(\"DATA DOWNLOAD SCRIPTS - FULL DISCUSSION\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "history = result_download_scripts.get_history()\n",
                "for turn in history:\n",
                "    role = turn.role\n",
                "    text = turn.parts[0].text if turn.parts else \"[No Content]\"\n",
                "    print(f\"\\n### {role}\\n\")\n",
                "    print(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the structured summary\n",
                "print(\"=\" * 50)\n",
                "print(\"DATA DOWNLOAD SCRIPTS - STRUCTURED SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "print(json.dumps(result_download_scripts.summary_structured, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the narrative summary\n",
                "print(\"=\" * 50)\n",
                "print(\"DATA DOWNLOAD SCRIPTS - NARRATIVE SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(result_download_scripts.summary_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. ETL Pipeline Development\n",
                "\n",
                "**Objective**: Design and implement ETL pipelines to transform raw data into the AlzKB knowledge graph format.\n",
                "\n",
                "**Participants**: The full team (PI, KG Engineer, Ontologist, Validation Scientist, Scientific Critic).\n",
                "\n",
                "**Focus Areas:**\n",
                "- Mapping raw data fields to ontology classes and properties\n",
                "- Entity resolution and normalization strategies\n",
                "- Creating nodes and relationships conforming to alzkb-ontology-v2.ttl\n",
                "- Handling data quality issues during transformation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ETL Pipeline Agenda defined.\n",
                        "==================================================\n",
                        "Task: Build a scalable, retrieval-optimized Knowledge Graph for Alzheimer's Disease research.\n",
                        "--- PREVIOUS PHASES CONTEXT ---\n",
                        "--- END PREVIOUS CONTEXT ---\n",
                        "\n",
                        "\n",
                        "TASK: Design the ETL Pipeline strategy to transform raw data into AlzKB Knowledge Graph triples.\n",
                        "\n",
                        "AVAILABLE RAW DATA (downloaded in previous phase):\n",
                        "1. GWAS Catalog (../data/raw/gwas_catalog/) - Filtered AD associations with p<5e-8, N>10k\n",
                        "2. Reactome (../data/raw/reactome/) - Human pathways in BioPAX/OWL format\n",
                        "3. DisGeNET (../data/raw/disgenet/) - Gene-disease associations with Score>0.5\n",
                        "4. ChEMBL (../data/raw/chembl/) - Drug-target bioactivities for AD targets\n",
                        "5. Ensembl (../data/raw/ensembl/) - Variant annotations for GWAS SNPs\n",
                        "\n",
                        "TARGET ONTOLOGY: alzkb-ontology-v2.ttl with:\n",
                        "- Classes: Patient, Gene, Protein, Allele, Genotype, HypothesisAssociation\n",
                        "- Properties: has_genotype, variant_of, has_association_source/target, p_value, inference_confidence\n",
                        "- SHACL constraints requiring p-value and confidence on HypothesisAssociation nodes\n",
                        "\n",
                        "FOR EACH DATA SOURCE, THE TEAM MUST DEFINE:\n",
                        "\n",
                        "1. ENTITY MAPPING:\n",
                        "   - Which raw fields map to which ontology classes?\n",
                        "   - How to generate canonical URIs for entities?\n",
                        "   - Example: GWAS 'SNPS' column → :Allele nodes with :variant_of → :Gene\n",
                        "\n",
                        "2. RELATIONSHIP EXTRACTION:\n",
                        "   - What relationships can be derived from each source?\n",
                        "   - Should associations use direct edges or :HypothesisAssociation reification?\n",
                        "   - Confidence scoring strategy (p-value thresholds → LOW/MEDIUM/HIGH)\n",
                        "\n",
                        "3. ENTITY RESOLUTION:\n",
                        "   - How to resolve the same gene across sources (GWAS uses symbols, ChEMBL uses ChEMBL IDs)?\n",
                        "   - Cross-reference strategy: Ensembl Gene IDs as canonical identifiers?\n",
                        "   - Handling synonyms and aliases\n",
                        "\n",
                        "4. QUALITY GATES:\n",
                        "   - What validation checks during ETL?\n",
                        "   - SHACL validation before final insertion?\n",
                        "   - Logging and error handling for malformed records\n",
                        "\n",
                        "Each team member should contribute:\n",
                        "- KG Engineer: Pipeline architecture, code structure, performance considerations\n",
                        "- Ontologist: Mapping specifications, URI minting conventions, semantic consistency\n",
                        "- Validation Scientist: Quality metrics, SHACL integration, RAG-readiness of output\n",
                        "- Scientific Critic: Identify gaps, challenge assumptions, ensure scientific rigor\n",
                        "\n",
                        "OUTPUT GOAL: A detailed ETL design document with:\n",
                        "- Source-to-ontology mapping tables\n",
                        "- Entity resolution strategy\n",
                        "- Pipeline architecture diagram (conceptual)\n",
                        "- Priority order for implementation\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Get context from previous phases\n",
                "prev_context = context.get_previous_context([\"ontology_creation\", \"data_resources\", \"download_scripts\"])\n",
                "\n",
                "# Define the Agenda for ETL Pipeline Development\n",
                "etl_pipeline_agenda = f\"\"\"{BACKGROUND_PROMPT}\n",
                "{prev_context}\n",
                "\n",
                "TASK: Design the ETL Pipeline strategy to transform raw data into AlzKB Knowledge Graph triples.\n",
                "\n",
                "AVAILABLE RAW DATA (downloaded in previous phase):\n",
                "1. GWAS Catalog (../data/raw/gwas_catalog/) - Filtered AD associations with p<5e-8, N>10k\n",
                "2. Reactome (../data/raw/reactome/) - Human pathways in BioPAX/OWL format\n",
                "3. DisGeNET (../data/raw/disgenet/) - Gene-disease associations with Score>0.5\n",
                "4. ChEMBL (../data/raw/chembl/) - Drug-target bioactivities for AD targets\n",
                "5. Ensembl (../data/raw/ensembl/) - Variant annotations for GWAS SNPs\n",
                "\n",
                "TARGET ONTOLOGY: alzkb-ontology-v2.ttl with:\n",
                "- Classes: Patient, Gene, Protein, Allele, Genotype, HypothesisAssociation\n",
                "- Properties: has_genotype, variant_of, has_association_source/target, p_value, inference_confidence\n",
                "- SHACL constraints requiring p-value and confidence on HypothesisAssociation nodes\n",
                "\n",
                "FOR EACH DATA SOURCE, THE TEAM MUST DEFINE:\n",
                "\n",
                "1. ENTITY MAPPING:\n",
                "   - Which raw fields map to which ontology classes?\n",
                "   - How to generate canonical URIs for entities?\n",
                "   - Example: GWAS 'SNPS' column → :Allele nodes with :variant_of → :Gene\n",
                "\n",
                "2. RELATIONSHIP EXTRACTION:\n",
                "   - What relationships can be derived from each source?\n",
                "   - Should associations use direct edges or :HypothesisAssociation reification?\n",
                "   - Confidence scoring strategy (p-value thresholds → LOW/MEDIUM/HIGH)\n",
                "\n",
                "3. ENTITY RESOLUTION:\n",
                "   - How to resolve the same gene across sources (GWAS uses symbols, ChEMBL uses ChEMBL IDs)?\n",
                "   - Cross-reference strategy: Ensembl Gene IDs as canonical identifiers?\n",
                "   - Handling synonyms and aliases\n",
                "\n",
                "4. QUALITY GATES:\n",
                "   - What validation checks during ETL?\n",
                "   - SHACL validation before final insertion?\n",
                "   - Logging and error handling for malformed records\n",
                "\n",
                "Each team member should contribute:\n",
                "- KG Engineer: Pipeline architecture, code structure, performance considerations\n",
                "- Ontologist: Mapping specifications, URI minting conventions, semantic consistency\n",
                "- Validation Scientist: Quality metrics, SHACL integration, RAG-readiness of output\n",
                "- Scientific Critic: Identify gaps, challenge assumptions, ensure scientific rigor\n",
                "\n",
                "OUTPUT GOAL: A detailed ETL design document with:\n",
                "- Source-to-ontology mapping tables\n",
                "- Entity resolution strategy\n",
                "- Pipeline architecture diagram (conceptual)\n",
                "- Priority order for implementation\n",
                "\"\"\"\n",
                "\n",
                "print(\"ETL Pipeline Agenda defined.\")\n",
                "print(\"=\" * 50)\n",
                "print(etl_pipeline_agenda)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting ETL Pipeline Development Meeting...\n",
                        "==================================================\n",
                        "Starting team meeting on: ETL Pipeline Development\n",
                        "\n",
                        "--- Round 1/2 ---\n",
                        "\n",
                        ">> Principal Investigator (Alzheimer's KG):\n",
                        "**Principal Investigator (PI)**\n",
                        "\n",
                        "\"Thank you, everyone. Let’s bring this to order. We have a robust set of raw data, but raw data is not knowledge until it is contextualized with high precision. In Alz...\n",
                        "\n",
                        ">> Data Ingestion & Quality Engineer:\n",
                        "**Data Ingestion & Quality Engineer**\n",
                        "\n",
                        "\"Agreed, PI. The 'Bronze/Silver/Gold' architecture is exactly the right approach for maintaining the audit trail we need. If we try to map raw CSVs directly to R...\n",
                        "\n",
                        ">> Semantic Knowledge Architect:\n",
                        "**Semantic Knowledge Architect**\n",
                        "\n",
                        "\"I agree with the pipeline architecture, but we must refine the **semantic schema** to ensure this graph is computationally reasoned over, not just queried. A clean p...\n",
                        "\n",
                        ">> RAG & Validation Scientist:\n",
                        "**RAG & Validation Scientist**\n",
                        "\n",
                        "\"I endorse the structural rigor proposed by the Architect and Engineer. However, a structurally perfect graph can still be **semantically silent** to an LLM if we don't...\n",
                        "\n",
                        ">> Scientific Critic (AlzKB):\n",
                        "**Scientific Critic**\n",
                        "\n",
                        "\"Hold on. While the architecture is sound, I see two potential **fatal flaws** in the scientific logic regarding the GWAS data processing. If we build the pipeline exactly as de...\n",
                        "\n",
                        "--- Round 2/2 ---\n",
                        "\n",
                        ">> Principal Investigator (Alzheimer's KG):\n",
                        "**Principal Investigator (PI)**\n",
                        "\n",
                        "\"Excellent catch, Scientific Critic. The 'Risk Allele Trap' is exactly the kind of silent failure that creates 'zombie graphs'—technically functional but biologically ...\n",
                        "\n",
                        ">> Data Ingestion & Quality Engineer:\n",
                        "**Data Ingestion & Quality Engineer**\n",
                        "\n",
                        "\"Understood, PI. I have updated the technical specifications to reflect these critical adjustments. The 'Risk Allele Trap' and 'Pruning' issues are now top prior...\n",
                        "\n",
                        ">> Semantic Knowledge Architect:\n",
                        "**Semantic Knowledge Architect**\n",
                        "\n",
                        "\"I approve the Data Engineer's logic, but I must formally register the schema definitions to support these changes. If these properties are not defined in the T-Box (...\n",
                        "\n",
                        ">> RAG & Validation Scientist:\n",
                        "**RAG & Validation Scientist**\n",
                        "\n",
                        "\"The schema and parsing logic are solid, but I have one final intervention regarding the **Text-to-Graph** bridge. We are building this for an AI system to consume, not...\n",
                        "\n",
                        ">> Scientific Critic (AlzKB):\n",
                        "**Scientific Critic**\n",
                        "\n",
                        "\"I have reviewed the updated specifications. The logic for GWAS Allele Harmonization and the Reactome hierarchy is now scientifically defensible. The 'Risk Allele Trap' has been...\n",
                        "\n",
                        "--- Generating Summaries ---\n",
                        "\n",
                        ">> SUMMARY:\n",
                        "**Principal Investigator (PI)**\n",
                        "\n",
                        "\"Team, this session has been exemplary. We have successfully transformed a generic data aggregation plan into a rigorous scientific instrument. By addressing the 'Risk Allele Trap,' enforcing 'Curated-Only' filters, and designing for 'RAG-Readiness,' we have ensured ...\n",
                        "==================================================\n",
                        "Meeting Complete.\n"
                    ]
                }
            ],
            "source": [
                "# Run the Team Meeting\n",
                "print(\"Starting ETL Pipeline Development Meeting...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "result_etl_pipeline = run_meeting(\n",
                "    meeting_type=\"team\",\n",
                "    agenda=etl_pipeline_agenda,\n",
                "    topic=\"ETL Pipeline Development\",\n",
                "    team_lead=PRINCIPAL_INVESTIGATOR,\n",
                "    team_members=TEAM_MEMBERS,\n",
                "    num_rounds=2,  # 2 rounds for detailed technical discussion\n",
                "    model_name=MODEL_PRO\n",
                ")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Meeting Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Phase 'etl_pipeline' saved.\n",
                        "Storage location: ..\\discussions\\etl_pipeline\n",
                        "Phases in context: ['etl_pipeline']\n"
                    ]
                }
            ],
            "source": [
                "# Store the result in the context\n",
                "context.add_result(\"etl_pipeline\", result_etl_pipeline)\n",
                "\n",
                "print(f\"Phase 'etl_pipeline' saved.\")\n",
                "print(f\"Storage location: {context.storage_dir / 'etl_pipeline'}\")\n",
                "print(f\"Phases in context: {context.list_phases()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the structured summary\n",
                "print(\"=\" * 50)\n",
                "print(\"ETL PIPELINE - STRUCTURED SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "print(json.dumps(result_etl_pipeline.summary_structured, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View the narrative summary\n",
                "print(\"=\" * 50)\n",
                "print(\"ETL PIPELINE - NARRATIVE SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(result_etl_pipeline.summary_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View full discussion for detailed design decisions\n",
                "print(\"=\" * 50)\n",
                "print(\"ETL PIPELINE - FULL DISCUSSION\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "history = result_etl_pipeline.get_history()\n",
                "for turn in history:\n",
                "    role = turn.role\n",
                "    text = turn.parts[0].text if turn.parts else \"[No Content]\"\n",
                "    print(f\"\\n### {role}\\n\")\n",
                "    print(text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Next: Validation & Quality Gates\n",
                "\n",
                "With ETL design complete, the subsequent phases will:\n",
                "\n",
                "1. **Phase 7: Validation & Quality Gates** - Implement SHACL validation and quality checks\n",
                "2. **Phase 8: Graph Database Deployment** - Load transformed data into Neo4j/GraphDB\n",
                "3. **Phase 9: RAG Integration** - Optimize the graph for retrieval-augmented generation"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "virtual_lab",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
