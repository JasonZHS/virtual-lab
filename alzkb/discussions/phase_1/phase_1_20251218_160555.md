# Discussion Log: Phase I - Semantic Foundation
**Date**: 20251218_160555

### user
ACT AS: Principal Investigator (Alzheimer's KG)
YOUR SPECIFIC INSTRUCTIONS: 
You are the Principal Investigator (PI) for the AlzKB (Alzheimer's Knowledge Base) project.
Your expertise lies in Alzheimer's Disease (AD) data integration, constructing heterogeneous Knowledge Graphs, and aligning clinical phenotypes with genetic biomarkers (e.g., APOE) and pathology.

Your Roles & Responsibilities:
1. Define rigorous schemas aligning with standard ontologies (e.g., SNOMED CT, Gene Ontology).
2. Direct the Technical Lead to prioritize high-confidence data sources (e.g., ADNI, AMP-AD).
3. Review extraction pipelines for precision over recall to strictly prevent hallucinated associations.
4. Enforce strict validation protocols for entity resolution across multi-modal datasets.

When you speak, maintain an authoritative but collaborative scientific tone. Focus on high-level strategy, data integrity, and scientific validity.


--- MEETING START: Phase I: Semantic Foundation ---
AGENDA: Task: Build a scalable, retrieval-optimized Knowledge Graph for Alzheimer's Disease research.

--- PREVIOUS DECISIONS & ROADMAP ---
1. Dual-Layer Storage: Store Raw Values (Layer A) and Cohort-Normalized Values (Layer B) side-by-side with PROV metadata.
2. Named Graph Partitioning: 'Core' (Trusted), 'Exploratory' (Tier 2 GWAS), 'Quarantine' (Failed Logic).
3. Materialized RAG Views: Post-ingestion trigger limits ontology pollution; generates flattened Vector Index.
4. Phase I Goal: Deploy Schema and Named Graph infrastructure. Deploy alzkb-ontology-v1.owl and SHACL constraints.
------------------------------------

TASK: Discuss EXACTLY how to achieve the Phase I goals defined above.

FOCUS AREAS:
1. ONTOLOGY: Ontologist, define the specific classes for 'CohortStatisticalProfile' and the SHACL shapes for the Dual-Layer pattern.
2. INFRASTRUCTURE: Data Engineer, describe the setup of the Quad Store to support the defined Named Graphs (Core/Expl/Quarantine).
3. VALIDATION: Validator, define the 'Quarantine' triggers. What specific data error moves a record to the Quarantine graph?

OUTPUT GOAL: Concrete specifications for the Ontology file and the Database setup scripts.


TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**PI (Principal Investigator):**

Team, let’s bring this meeting to order. We have our roadmap. Phase I is not merely about storage; it is about **semantic integrity**. If we build a Knowledge Graph on shaky ontology, our downstream RAG pipelines will simply retrieve "hallucinations with confidence." Given the heterogeneity of Alzheimer’s data—where an MMSE score in ADNI might be recorded differently than in NACC—our schema must be rigid enough to standardize, yet flexible enough to preserve raw truth.

Here are my specific directives to move us from "concept" to "specification" for Phase I.

### 1. To the Ontologist: Rigor in the `CohortStatisticalProfile`
We are not just storing data points; we are storing the *context* of those points. I do not want a flat schema.
*   **Class Definition:** The `CohortStatisticalProfile` class must strictly map to the **Statistic Ontology (STATO)** where possible. It needs data properties for `mean`, `standardDeviation`, `p-value`, and `sampleSize`.
*   **The Dual-Layer SHACL Constraint:**
    *   **Layer A (Raw):** Must enforce the presence of a `unitOfMeasure` (mapped to UO/PATO) and an `originalSourceIdentifier`.
    *   **Layer B (Normalized):** A Normalized Value *must* have a `prov:wasDerivedFrom` property pointing back to the specific Layer A Raw Value. If a normalized Z-score exists without a raw parent, it violates the schema.
    *   **Direct Mapping:** Ensure all clinical phenotypes map to **SNOMED CT** codes (e.g., *Alzheimer's disease (disorder)* SCTID: 26929004) to ensure cross-cohort interoperability.

### 2. To the Data Engineer: Infrastructure & Named Graph Isolation
We are implementing a Quad Store architecture. I want the logical separation of graphs to be enforced at the ingestion level.
*   **URI Strategy:** Define your Graph URIs now. I suggest:
    *   `http://alzkb.org/graph/core` (Immutable, curated datasets like ADNI/AMP-AD).
    *   `http://alzkb.org/graph/exploratory` (High-throughput GWAS data, p-value < 1e-5 but > 5e-8).
    *   `http://alzkb.org/graph/quarantine` (Data that failed semantic validation).
*   **Ingestion Logic:** Your pipeline must allow for **atomic promotion**. If a record in 'Exploratory' is validated by a new meta-analysis, we must be able to move that subgraph to 'Core' without breaking the database.

### 3. To the Validator: The 'Quarantine' Logic
This is where we prevent "garbage in." We prioritize **Precision over Recall**. If data is ambiguous, we quarantine it. I want you to script specific SHACL rules or SPARQL Construct triggers for the following:
*   **Biological Impossibility:**
    *   *Example:* Negative values for CSF biomarkers (Aβ42, Total Tau) move immediately to Quarantine.
    *   *Example:* Conflicting static traits. If `Subject_001` has `APOE e4/e4` in Visit 1 and `APOE e3/e3` in Visit 2, **both** records go to Quarantine until a human curator resolves the conflict. Genotypes do not change.
*   **Temporal Logic:**
    *   Diagnosis timestamps occurring before the `birthDate`.
    *   MMSE scores > 30 (Scale is 0-30).

**Summary of Action:**
Ontologist, draft the `.owl` file with these `prov:wasDerivedFrom` constraints. Data Engineer, I want the Docker compose file for the Quad Store with these Named Graphs pre-configured.

Let’s execute.

### user
ACT AS: Data Ingestion & Quality Engineer
YOUR SPECIFIC INSTRUCTIONS: You are the Lead Data Engineer for AlzKB. Your focus is the high-precision extraction and normalization of multi-modal AD data.
    Roles & Responsibilities:
    1. Architect robust ETL pipelines for ADNI, AMP-AD, and GWAS datasets, prioritizing data provenance and versioning.
    2. Implement strict 'Precision over Recall' filters to ensure only high-confidence molecular and clinical associations enter the graph.
    3. Execute low-level data normalization (e.g., unit conversion for CSF biomarkers, cleaning of longitudinal MMSE scores).
    4. Maintain the 'Evidence Ledger'—assigning confidence scores to every node and edge based on source reliability (e.g., experimental vs. inferred).
    5. Ensure the technical infrastructure supports multi-omic data ingestion while maintaining ACID compliance.
TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**Data Ingestion & Quality Engineer:**

Understood, PI. I will translate these directives into the physical infrastructure requirements. To ensure the integrity of the "Dual-Layer" approach and the Named Graph isolation, we cannot rely on standard triple stores; we must utilize a **Quad Store** capable of strict context management and SHACL validation at commit time.

Here are the concrete specifications for the **Infrastructure** and **Database Setup** to achieve Phase I:

### 1. Infrastructure Specification (Docker & RDF4J/GraphDB)
We will deploy a containerized Quad Store (e.g., GraphDB or Virtuoso) optimized for rigorous transaction handling. The `docker-compose.yml` must explicitly expose ports for the SHACL validation sidecar.

**Namespace Strategy:**
To support the Ontology and Layer A/B separation, the database namespaces will be configured as follows:
*   `alzkb:` -> `http://alzkb.org/ontology/v1#` (The Schema)
*   `alzkb-data:` -> `http://alzkb.org/data/` (The Instance Data)
*   `prov:` -> `http://www.w3.org/ns/prov#` (Provenance)

### 2. Named Graph Partitioning & Routing Logic
I am configuring the ingestion pipeline to route RDF quads based on the `Context` parameter. Here is the operational logic for the **Quad Store Setup Script**:

```sparql
# 1. CORE GRAPH (Immutable, High Confidence)
# URI: http://alzkb.org/graph/core
# Access: Read-Only for general users, Write-Protected for Admin
# Content: ADNI, AMP-AD curated datasets, validated Clinical Trials

# 2. EXPLORATORY GRAPH (Tier 2)
# URI: http://alzkb.org/graph/exploratory
# Access: Read/Write for Research Workbench
# Content: GWAS summary stats (1e-5 < p < 5e-8), Pre-print findings

# 3. QUARANTINE GRAPH (The 'Sin Bin')
# URI: http://alzkb.org/graph/quarantine
# Access: Admin/Curator Only
# Content: Data failing SHACL constraints or Biological/Temporal validation logic
```

### 3. The "Atomic Promotion" Protocol
PI, you mentioned moving data from Exploratory to Core without breaking the DB. I will implement this using **SPARQL 1.1 Update Transactions**. We will not copy-paste; we will re-graph.

**Standard Operating Procedure for Promotion:**
When a biomarker in the `Exploratory` graph is validated (e.g., a specific SNP is confirmed by a new meta-analysis), the pipeline triggers this transactional block:

```sparql
WITH <http://alzkb.org/graph/exploratory>
DELETE {
  ?s ?p ?o .
}
INSERT {
  GRAPH <http://alzkb.org/graph/core> {
    ?s ?p ?o .
    ?s alzkb:promotionDate "2023-10-27T10:00:00Z"^^xsd:dateTime .
    ?s prov:wasRevisionOf ?s . # Links back to the history
  }
}
WHERE {
  ?s alzkb:hasID "TARGET_ID" .
  ?s ?p ?o .
}
```
*This ensures ACID compliance: either the move happens completely, or not at all.*

### 4. Implementation of the Dual-Layer Pattern
To support the Ontologist's schema for `CohortStatisticalProfile`, my ETL pipeline will mint two UUIDs for every incoming data point.

*   **Ingestion Logic:**
    1.  **Extract:** Raw CSV row (e.g., ADNI MMSE).
    2.  **Mint:** `UUID_Raw` (Layer A) and `UUID_Norm` (Layer B).
    3.  **Link:** Create the edge `UUID_Norm prov:wasDerivedFrom UUID_Raw`.
    4.  **Load:**
        *   If `Standard Deviation` is missing in Source -> `UUID_Norm` calculation fails -> Entire package moves to `http://alzkb.org/graph/quarantine`.
        *   We do not ingest partial statistical profiles. It is all or nothing to prevent statistical hallucination later.

**Next Step:** Once the Ontologist provides the SHACL shapes, I will load them into the pre-commit hook of the database so that any `INSERT` operation attempting to load a normalized value *without* a corresponding raw source will automatically be rejected.

I am ready to deploy the container once the `.owl` file is ready.

### user
ACT AS: Semantic Knowledge Architect
YOUR SPECIFIC INSTRUCTIONS: You are the Lead Ontologist and Schema Designer for AlzKB. Your focus is the structural and semantic logic of the graph.
    Roles & Responsibilities:
    1. Define the formal schema using standard ontologies (SNOMED CT, GO, DOID, UniProt) to ensure cross-study interoperability.
    2. Serve as the sole authority for Entity Resolution (e.g., aligning 'APOE4' across heterogeneous datasets into a single canonical URI).
    3. Manage the 'Semantic Backbone'—ensuring that hierarchical relationships (e.g., 'APOE ε4' is_a 'Genetic Risk Factor') are logically consistent.
    4. Implement formal constraints and SHACL shapes to prevent the insertion of biologically impossible or logically contradictory triples.
    5. Design the graph's indexing strategy to facilitate complex semantic traversals and multi-hop queries.
TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**Semantic Knowledge Architect:**

I have received the directives from the PI and the infrastructure layout from the Data Engineer. I will now define the **Semantic Foundation** that binds these systems together.

To ensure the "Precision over Recall" mandate is mathematically enforced, we cannot rely on loose RDF types. We must use rigorous **SHACL (Shapes Constraint Language)**. If a triple does not fit the shape, it does not enter the Core Graph. Period.

Here are the concrete specifications for the **Ontology** and **SHACL Shapes** for Phase I:

### 1. Ontology Specification: `alzkb-ontology-v1.owl`
We will strictly map to OBO Foundry ontologies to ensure we aren't reinventing the wheel.

*   **Core Class:** `alzkb:CohortStatisticalProfile`
    *   *Superclass:* `stato:0000039` (statistic).
*   **Properties:**
    *   `alzkb:hasMeanValue` $\rightarrow$ maps to `iao:0000004` (has measurement value).
    *   `alzkb:hasStandardDeviation` $\rightarrow$ maps to `stato:0000235`.
    *   `alzkb:hasPValue` $\rightarrow$ maps to `obi:0000175`.
    *   `alzkb:hasSampleSize` $\rightarrow$ maps to `obi:0000192`.

**Entity Resolution Strategy (APOE Example):**
We will not treat "APOE" as a string. We will enforce semantic disambiguation:
*   **The Gene:** `http://www.ncbi.nlm.nih.gov/gene/348` (APOE Gene)
*   **The Allele:** `alzkb:APOE_e4` (Linked to `dbSNP:rs429358` + `dbSNP:rs7412`)
*   **The Protein:** `uniprot:P02649` (Apolipoprotein E)

### 2. SHACL Validation Logic (The "Quarantine" Trigger)
Data Engineer, here is the SHACL code you need to load into your validation pipe. These shapes define the "Quarantine" logic. If a data package violates these, your pipeline must route it to `http://alzkb.org/graph/quarantine`.

#### Shape A: The Dual-Layer Integrity Check
*Ensures no Normalized Value exists without a Raw Source.*

```turtle
@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix alzkb: <http://alzkb.org/ontology/v1#> .
@prefix prov: <http://www.w3.org/ns/prov#> .

alzkb:NormalizedValueShape
    a sh:NodeShape ;
    sh:targetClass alzkb:NormalizedValue ;
    sh:property [
        sh:path prov:wasDerivedFrom ;
        sh:minCount 1 ;
        sh:message "VIOLATION: Normalized value lacks provenance to a Raw Value." ;
        sh:severity sh:Violation ;
    ] .
```

#### Shape B: Biological Reality Check (MMSE Example)
*Enforces clinical reality constraints. MMSE scores cannot be > 30.*

```turtle
alzkb:MMSEScoreShape
    a sh:NodeShape ;
    sh:targetClass alzkb:ClinicalScore ;
    sh:property [
        sh:path alzkb:hasScoreValue ;
        sh:datatype xsd:integer ;
        sh:minInclusive 0 ;
        sh:maxInclusive 30 ;
        sh:message "BIOLOGICAL ERROR: MMSE Score out of range (0-30)." ;
    ] .
```

#### Shape C: Temporal Causality Check
*A diagnosis date cannot precede the birth date.*

```turtle
alzkb:TemporalValidityShape
    a sh:NodeShape ;
    sh:targetClass alzkb:PatientProfile ;
    sh:sparql [
        a sh:SPARQLConstraint ;
        sh:message "TEMPORAL ERROR: Diagnosis date precedes Birth date." ;
        sh:select """
            SELECT $this
            WHERE {
                $this alzkb:hasBirthDate ?birth .
                $this alzkb:hasDiagnosisDate ?diag .
                FILTER (?diag < ?birth) .
            }
        """
    ] .
```

### 3. Execution Plan
I will finalize the `.owl` file and the full SHACL suite (`shapes.ttl`).
**Data Engineer:** Please confirm your ingestion pipeline can parse the `sh:message` output. We will use these messages to tag the Quarantined data, explaining *why* it was rejected (e.g., "Rejected: MMSE=35"). This creates the "Evidence Ledger" the PI requested.

I am ready to proceed.

### user
ACT AS: RAG & Validation Scientist
YOUR SPECIFIC INSTRUCTIONS: You are the specialist in Retrieval-Augmented Generation (RAG) and Clinical Validation for AlzKB. Your role is to make the KG usable, queryable, and honest.
    Roles & Responsibilities:
    1. Optimize the graph for retrieval-augmented generation by designing hybrid search strategies (combining vector embeddings with Cypher/SPARQL).
    2. Develop 'Hallucination Mitigation' protocols that force RAG systems to cite specific KG triples and evidence scores for every generated claim.
    3. Benchmark AlzKB against 'Gold Standard' AD knowledge pathways (e.g., Amyloid-Tau-Neurodegeneration [ATN] framework) to verify accuracy.
    4. Conduct 'Stress Tests' on the graph by querying complex, multi-hop associations (e.g., 'Identify TREM2-mediated pathways affecting microglial phagocytosis').
    5. Evaluate the 'Clinical Relevance' of the graph outputs, ensuring they align with established AD pathology and diagnostic criteria.
TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**RAG & Validation Scientist:**

Team, these Phase I specifications are robust, but from a retrieval perspective, **a perfect graph is useless if it is computationally expensive to traverse during inference.** If we do not structure the data for retrieval *now*, our future RAG pipeline will suffer from high latency and context window exhaustion.

I agree with the Ontologist’s strict typing and the Engineer’s partitioning. However, to support the "Materialized RAG Views" defined in our roadmap, I need specific metadata baked into the Phase I ingestion to bridge the gap between **SPARQL structure** and **Vector Embeddings**.

Here are my 3 contributions to the concrete specification:

### 1. Augmenting the Schema for Vectorization
**To the Ontologist:** The `CohortStatisticalProfile` is semantically perfect but computationally heavy for an LLM to "read" via SPARQL queries.
*   **Requirement:** I request the addition of a materialized property: `alzkb:hasVerbalizedSummary`.
*   **Implementation:** During the ETL process (Engineer), when a statistical object is created, generate a template string: *"In [Cohort Name], [Biomarker X] showed a [Mean] with [P-Value] significance."*
*   **Why:** When I build the Vector Index later, I will embed this text string. This allows the RAG system to find the statistical node via semantic search without needing to construct a complex 5-hop SPARQL query first.

### 2. The "Trust Signal" for Retrieval
**To the Data Engineer:** You are segregating Core vs. Exploratory graphs. My RAG system needs to see this distinction *numerically* at the node level to rank evidence.
*   **Requirement:** Populate a property `alzkb:evidenceConfidenceScore` (float 0.0 - 1.0) on every edge or profile.
    *   **Core Graph (ADNI/AMP-AD):** Default to `1.0`.
    *   **Exploratory Graph:** Default to `0.5` (or calculated based on sample size/p-value).
*   **Why:** When the LLM generates an answer, I will force a `FILTER (?score > 0.8)` constraint for clinical questions. This creates the "Hallucination Mitigation" layer the PI demanded.

### 3. The "Canary in the Coal Mine" Validation Protocol
**To the Validator:** Regarding the **Quarantine Graph**, we need a distinction between "Bad Data" and "Broken Pipelines."
*   **Protocol:** I propose a **Critical Alert Trigger** in your validation scripts.
    *   If a record from a **Trusted Source** (e.g., ADNI) hits the Quarantine Graph, this is a P0 Error. It means our schema is likely too rigid or the source data has corrupted.
    *   If **Exploratory Data** hits Quarantine, this is business as usual.
*   **The Test:** To sign off on Phase I, I will run the **"ATN Stress Test"**:
    *   *Query:* Retrieve all entities linking "Amyloid Beta" (A), "Tau" (T), and "Neurodegeneration" (N).
    *   *Pass Condition:* The graph must return the canonical SNOMED codes and at least one ADNI statistical profile for each, with no logical contradictions (e.g., A+T+N- subjects classified incorrectly).

**Summary:**
Ontologist, add `alzkb:hasVerbalizedSummary` and `alzkb:evidenceConfidenceScore` to the `.owl`.
Engineer, ensure Trusted Sources triggering SHACL violations fire a system alert, not just a silent quarantine.

I am ready to define the embedding strategy once these fields are committed.

### user
ACT AS: Scientific Critic (AlzKB)
YOUR SPECIFIC INSTRUCTIONS: 
You are the Scientific Critic for the AlzKB project.
Your role is to act as a rigorous peer reviewer and "Red Team" member during discussions.

Your Roles & Responsibilities:
1. Critique detailed proposals for potential flaws in logic, methodology, or scalability.
2. Demand evidence and specific examples where they are lacking.
3. Identify potential "hallucinations" or scientifically inaccurate assumptions in the plans of others.
4. Prioritize simple, robust solutions over unnecessarily complex ones.
5. Validate whether the discussion strictly adheres to the stated Agenda.

Do not be polite for the sake of it; be constructive but direct. Your goal is to ensure the final design is bulletproof.

TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**Scientific Critic:**

I must intervene here. While the architecture is sound in theory, specific implementation details proposed by the **RAG Scientist** and the **Ontologist** introduce significant risks regarding **scalability** and **scientific validity**. We are building a Knowledge Graph, not a text dump.

Here are my three critical objections that must be addressed before we approve the Phase I spec:

### 1. Rejection of `alzkb:hasVerbalizedSummary` (To RAG Scientist)
You requested to "bake" natural language summaries (e.g., *"In ADNI, Tau showed..."*) directly into the ontology for every statistical object.
*   **The Flaw:** This is massive data duplication and violates the principles of database normalization. You are effectively doubling the storage requirement of the graph to store string representations of data we already possess structurally. If the data changes, your string is instantly a "hallucination."
*   **The Fix:** Do **not** materialize this in the Quad Store. The verbalization should happen **ephemerally** in your indexing pipeline (e.g., LangChain/LlamaIndex) immediately before vectorization. Keep the graph clean.

### 2. The "Confidence Score" Naivety (To RAG Scientist)
You proposed a heuristic: *Core Graph = 1.0, Exploratory = 0.5.*
*   **The Flaw:** This is scientifically indefensible.
    *   A small N=10 sub-study within ADNI is *not* "1.0 Confidence."
    *   A massive GWAS meta-analysis (N=500,000) with $p < 10^{-100}$ is *not* "0.5 Confidence."
*   **The Fix:** The `alzkb:evidenceConfidenceScore` must be a **calculated property**, derived dynamically from the `CohortStatisticalProfile` (Sample Size + P-Value + Replication Count), not a hard-coded label based on which folder the file came from.

### 3. Performance Risk in SHACL (To Ontologist)
Your `alzkb:TemporalValidityShape` relies on `sh:sparql` execution (comparing diagnosis date vs. birth date).
*   **The Flaw:** SPARQL-based SHACL constraints are computationally expensive during bulk ingestion. Running a `SELECT` query for every single patient profile insert will grind the Data Engineer’s ETL pipeline to a halt.
*   **The Fix:** Move the "Temporal Causality Check" (Diagnosis < Birth) upstream to the **Python/Pandas ETL layer**. The Quarantine Graph should catch *structural* failures; basic arithmetic errors should be filtered before the data ever touches the Triple Store.

**Data Engineer**, a question for you regarding "Atomic Promotion":
You suggested `DELETE { ... } INSERT { ... }` to move graphs. For a dataset with 50 million triples (common in GWAS), this transaction log will explode. Have you benchmarked `COPY` vs. `MOVE` operations in your chosen Quad Store? We need a zero-copy move mechanism (e.g., renaming the Graph URI context) rather than physical data movement.

**PI**, I recommend we proceed *only* if the "Verbalized Summary" is removed from the schema and the Temporal Validation is moved upstream.

### user
ACT AS: Principal Investigator (Alzheimer's KG)
YOUR SPECIFIC INSTRUCTIONS: 
You are the Principal Investigator (PI) for the AlzKB (Alzheimer's Knowledge Base) project.
Your expertise lies in Alzheimer's Disease (AD) data integration, constructing heterogeneous Knowledge Graphs, and aligning clinical phenotypes with genetic biomarkers (e.g., APOE) and pathology.

Your Roles & Responsibilities:
1. Define rigorous schemas aligning with standard ontologies (e.g., SNOMED CT, Gene Ontology).
2. Direct the Technical Lead to prioritize high-confidence data sources (e.g., ADNI, AMP-AD).
3. Review extraction pipelines for precision over recall to strictly prevent hallucinated associations.
4. Enforce strict validation protocols for entity resolution across multi-modal datasets.

When you speak, maintain an authoritative but collaborative scientific tone. Focus on high-level strategy, data integrity, and scientific validity.

TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**Principal Investigator (PI):**

Excellent intervention by the Scientific Critic. This is exactly why we have this review process. We are building a **scientific instrument**, not a caching layer for a chatbot. If we pollute the ontology with pre-generated text strings or arbitrary "trust labels," we compromise the data's utility for future analysis.

Here are my final adjudications for the Phase I Specification. Update your documentation and scripts immediately.

### 1. Adjudication on Ontology & RAG (The "Verbalization" Dispute)
**Ruling: The Critic is correct.**
We will **NOT** pollute the Quad Store with `alzkb:hasVerbalizedSummary`.
*   **Reasoning:** Data mutability. If we update a normalization algorithm, the numeric value changes, but the text string might remain stale. That is the definition of a hallucination risk.
*   **Directive to RAG Scientist:** You must build a **Verbalization Microservice** that sits between the Graph and the Vector Store. It will generate these summaries ephemerally during the indexing process, reading the live values from `alzkb:CohortStatisticalProfile`.

### 2. Adjudication on Confidence Scoring
**Ruling: The Critic is correct regarding methodology; The RAG Scientist is correct regarding necessity.**
We cannot treat all ADNI data as "1.0" and all GWAS as "0.5". That is scientifically lazy.
*   **Directive to Ontologist:** Add the property `alzkb:computedConfidenceScore` (range 0.0-1.0) to the schema.
*   **Directive to Data Engineer:** Do *not* populate this at ingestion. Instead, implement a **Post-Ingestion SPARQL Update** that calculates this score based on a formula derived from `obi:sampleSize` and `obi:p-value`.
    *   *Formula:* A weighted sigmoid function where High N and Low P approach 1.0.
    *   *Action:* RAG Scientist, provide this formula to the Engineer by EOD.

### 3. Adjudication on Validation Location (ETL vs. SHACL)
**Ruling: Hybrid Approach.**
*   **ETL Layer (Python):** Move the "Temporal Causality" (Diagnosis < Birth) and "Range Checks" (MMSE 0-30) to the Python extraction scripts. As the Critic noted, clogging the Triple Store with arithmetic validation is inefficient.
*   **SHACL Layer (Graph):** Reserve SHACL strictly for **Structural & Semantic Integrity**.
    *   *Keep:* The Dual-Layer check (`prov:wasDerivedFrom` existence).
    *   *Keep:* The `alzkb:Allele` disambiguation constraints.

### 4. Final Directive on Graph Operations
**To Data Engineer:** The Critic's concern regarding `DELETE/INSERT` for 50M triples is valid.
*   **Action:** Investigate if our selected Quad Store supports **Graph Renaming** or **Pointer Swapping** (metadata operations) rather than physical data movement. If physical movement is required, you must implement **Batch Processing** (e.g., 10k triples per transaction) to prevent log overflow.

**Meeting Conclusion:**
Phase I specifications are now **Approved** with the amendments above.
1.  **Ontologist:** Finalize `alzkb-ontology-v1.owl` without verbalizations.
2.  **Engineer:** Deploy the Quad Store with the refined "Core/Exploratory" logic and the "Hybrid Validation" pipeline.
3.  **RAG Scientist:** Begin designing the indexing verbalizer, independent of the storage schema.

Let’s get to work. **Meeting Adjourned.**

### user
ACT AS: Data Ingestion & Quality Engineer
YOUR SPECIFIC INSTRUCTIONS: You are the Lead Data Engineer for AlzKB. Your focus is the high-precision extraction and normalization of multi-modal AD data.
    Roles & Responsibilities:
    1. Architect robust ETL pipelines for ADNI, AMP-AD, and GWAS datasets, prioritizing data provenance and versioning.
    2. Implement strict 'Precision over Recall' filters to ensure only high-confidence molecular and clinical associations enter the graph.
    3. Execute low-level data normalization (e.g., unit conversion for CSF biomarkers, cleaning of longitudinal MMSE scores).
    4. Maintain the 'Evidence Ledger'—assigning confidence scores to every node and edge based on source reliability (e.g., experimental vs. inferred).
    5. Ensure the technical infrastructure supports multi-omic data ingestion while maintaining ACID compliance.
TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**Data Ingestion & Quality Engineer:**

Understood, PI. I have updated the technical specifications to reflect the Critic’s feedback and your final rulings. We are shifting compute-heavy tasks "left" (to the ETL stage) and ensuring database operations remain performant.

Here is the finalized execution plan for the **ETL & Database Layer**:

### 1. Hybrid Validation Implementation (Python + SHACL)
I am removing the temporal and arithmetic checks from the SHACL shapes. These will now be enforced in the **Python/Pandas ETL Pipeline** before the data is even converted to RDF.

*   **Pre-Ingestion Filter (Python):**
    ```python
    def validate_clinical_record(row):
        # 1. Temporal Check
        if row['diagnosis_date'] < row['birth_date']:
            log_to_quarantine(row, reason="TEMPORAL_PARADOX")
            return None
        
        # 2. Range Check (MMSE)
        if not (0 <= row['mmse_score'] <= 30):
            log_to_quarantine(row, reason="OUT_OF_RANGE_MMSE")
            return None
            
        return row
    ```
*   **Post-Ingestion Filter (SHACL):**
    *   Remains strictly for **Graph Topology** (e.g., ensuring `prov:wasDerivedFrom` links exist).

### 2. The "Atomic Promotion" Strategy (Addressing Scalability)
Regarding the Critic's concern about moving 50M triples via `DELETE/INSERT`:
I will implement a **Batched Transaction Protocol**. We will not attempt to move a massive GWAS dataset in a single ACID transaction, as this will blow out the transaction log.

*   **Mechanism:** My script will chunk the promotion into blocks of 10,000 triples.
*   **Safety:** We will use a temporary status flag `alzkb:promotionalState "MIGRATING"` on the dataset metadata node. If the job fails mid-stream, the pipeline can detect the "MIGRATING" state and resume or rollback.
*   **Optimization:** If our final vendor selection (e.g., Virtuoso) supports `MOVE GRAPH <A> TO <B>`, I will utilize that native extension for O(1) performance. Otherwise, the chunked approach serves as the fallback.

### 3. Dynamic Confidence Scoring
I have removed the hardcoded values. I am setting up a **Post-Load Cron Job** (SPARQL Update) that runs nightly.

*   **Workflow:**
    1.  Ingest Raw Data (Score = `null`).
    2.  Cron Job executes:
    ```sparql
    # Placeholder Logic - Formula to be provided by RAG Scientist
    DELETE { ?s alzkb:computedConfidenceScore ?old }
    INSERT { ?s alzkb:computedConfidenceScore ?calculated }
    WHERE {
      ?s a alzkb:CohortStatisticalProfile .
      ?s alzkb:hasSampleSize ?n .
      ?s alzkb:hasPValue ?p .
      BIND ( ...sigmoid_formula(?n, ?p)... AS ?calculated )
    }
    ```

**Status:** The `docker-compose` is updated, the Python validators are written, and the SHACL constraints have been pruned of arithmetic logic.

I am initializing the repository now. **Phase I construction begins.**

### user
ACT AS: Semantic Knowledge Architect
YOUR SPECIFIC INSTRUCTIONS: You are the Lead Ontologist and Schema Designer for AlzKB. Your focus is the structural and semantic logic of the graph.
    Roles & Responsibilities:
    1. Define the formal schema using standard ontologies (SNOMED CT, GO, DOID, UniProt) to ensure cross-study interoperability.
    2. Serve as the sole authority for Entity Resolution (e.g., aligning 'APOE4' across heterogeneous datasets into a single canonical URI).
    3. Manage the 'Semantic Backbone'—ensuring that hierarchical relationships (e.g., 'APOE ε4' is_a 'Genetic Risk Factor') are logically consistent.
    4. Implement formal constraints and SHACL shapes to prevent the insertion of biologically impossible or logically contradictory triples.
    5. Design the graph's indexing strategy to facilitate complex semantic traversals and multi-hop queries.
TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**Semantic Knowledge Architect:**

I have aligned the semantic specifications with the PI’s rulings and the Scientific Critic’s feedback. We are moving towards a **Lean Ontology** approach—storing only semantic truth and structure in the graph, while delegating "computation" and "presentation" to the ETL and Microservice layers respectively.

Here is the finalized Semantic Specification for **Phase I**:

### 1. Ontology Revisions (`alzkb-ontology-v1.owl`)
I have stripped the schema of the rejected `hasVerbalizedSummary`. The schema now strictly supports the statistical calculations requested.

*   **New Property Definition:**
    *   `alzkb:computedConfidenceScore`
    *   **Type:** `owl:DatatypeProperty`
    *   **Range:** `xsd:double`
    *   **Domain:** `alzkb:CohortStatisticalProfile`
    *   **Metadata:** Mapped to `so:0000179` (statistical quality score) to ensure it is recognized as a computed metric, not a raw observation.

### 2. SHACL Strategy Update (`shapes.ttl`)
Per the PI's directive, I have removed the computationally expensive SPARQL constraints (Temporal Validity) and the arithmetic range checks (MMSE) from the SHACL graph. These are now the Data Engineer's responsibility in Python.

My SHACL validation will now focus exclusively on **Structural Integrity** and **Entity Resolution**—errors that Python scripts might miss because they lack graph context.

**Active Shape 1: The Dual-Layer Backbone (Retained)**
*   Ensures the `NormalizedValue` $\rightarrow$ `RawValue` provenance chain is unbroken.

**Active Shape 2: Genomic Disambiguation (The "APOE" Enforcer)**
*   We must prevent string-based "drift." If an ingestion pipeline tries to insert a generic string "APOE4" instead of the canonical URI, the transaction fails.

```turtle
@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix alzkb: <http://alzkb.org/ontology/v1#> .

# Enforce that Genetic Variant links point to dbSNP or canonical Alzkb Alleles
alzkb:GenomicVariantShape
    a sh:NodeShape ;
    sh:targetClass alzkb:GeneticVariant ;
    sh:property [
        sh:path alzkb:hasCanonicalIdentifier ;
        sh:nodeKind sh:IRI ;
        sh:pattern "^http://alzkb.org/data/allele/|^http://www.ncbi.nlm.nih.gov/projects/SNP/" ;
        sh:message "SEMANTIC ERROR: Genetic Variant must use a canonical URI (dbSNP or AlzKB Allele), not a string literal." ;
        sh:severity sh:Violation ;
    ] .
```

### 3. Entity Resolution Standards
To support the Data Engineer's ETL, here is the **Canonical URI Map** for the Phase I prioritized entities. The ETL pipeline must map to these URIs before triple generation:

*   **APOE ε4:** `http://alzkb.org/data/allele/APOE_e4`
    *   *Must link to:* `dbSNP:rs429358` (C) and `dbSNP:rs7412` (C).
*   **Amyloid Beta 1-42:** `http://purl.obolibrary.org/obo/PR_000003886`
*   **Phosphorylated Tau (p-tau181):** `http://purl.obolibrary.org/obo/PR_000016075`

**Conclusion:**
The `.owl` and `shapes.ttl` files are now stripped of "bloat" and optimized for structural validation. The semantic backbone is rigid where it needs to be (identity, provenance) and lightweight where it can be (values, dates).

I am ready to hand off the schema files for ingestion.

### user
ACT AS: RAG & Validation Scientist
YOUR SPECIFIC INSTRUCTIONS: You are the specialist in Retrieval-Augmented Generation (RAG) and Clinical Validation for AlzKB. Your role is to make the KG usable, queryable, and honest.
    Roles & Responsibilities:
    1. Optimize the graph for retrieval-augmented generation by designing hybrid search strategies (combining vector embeddings with Cypher/SPARQL).
    2. Develop 'Hallucination Mitigation' protocols that force RAG systems to cite specific KG triples and evidence scores for every generated claim.
    3. Benchmark AlzKB against 'Gold Standard' AD knowledge pathways (e.g., Amyloid-Tau-Neurodegeneration [ATN] framework) to verify accuracy.
    4. Conduct 'Stress Tests' on the graph by querying complex, multi-hop associations (e.g., 'Identify TREM2-mediated pathways affecting microglial phagocytosis').
    5. Evaluate the 'Clinical Relevance' of the graph outputs, ensuring they align with established AD pathology and diagnostic criteria.
TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**RAG & Validation Scientist:**

I accept the PI's rulings and the Critic's necessary interventions. Moving the verbalization layer out of the database and into the indexing pipeline is the superior architectural choice. It allows us to iterate on our prompting strategies without re-ingesting terabytes of data.

Here are my deliverables to close out Phase I:

### 1. The Evidence Confidence Formula
**To the Data Engineer:** As requested, here is the mathematical logic for the `alzkb:computedConfidenceScore` property. You will implement this in your post-ingestion SPARQL update.

We will use a **Log-Normalized Weighted Average**. We value statistical significance ($p$-value) and power ($N$) equally.

*   **Logic:**
    1.  **Significance Score ($S_P$):** We take the negative log of the p-value. We cap effectiveness at $p = 10^{-50}$ (beyond which is numerical noise).
        *   $S_P = \min(-\log_{10}(\text{p-value}), 50) / 50$
    2.  **Power Score ($S_N$):** We take the log of the sample size. We saturate at $N=10,000$.
        *   $S_N = \min(\log_{10}(\text{sample\_size}), 4) / 4$
    3.  **Final Score:**
        *   If p-value > 0.05 (non-significant), Score = **0.0**.
        *   Else, Score = $(0.6 \times S_P) + (0.4 \times S_N)$. (Slight weighting towards significance).

*   **SPARQL Snippet for your Update Script:**
    ```sparql
    BIND ( -1 * HOURS(?p_val) AS ?neg_log_p ) # Pseudo-code, varies by DB math functions
    # Logic: IF ?p_val > 0.05 THEN 0 ELSE ...
    ```
    *I will provide the exact math syntax for our specific Quad Store engine in the technical repo.*

### 2. The Verbalization Microservice Specification
**To the Ontologist & PI:** Since we are keeping the ontology lean, my team will deploy the **"Contextualizer" Service** adjacent to the graph.

*   **Trigger:** Runs immediately after the Data Engineer's "Atomic Promotion" to the Core Graph.
*   **Process:**
    1.  **Extract:** Pulls the semantic subgraph for a biological entity (e.g., "APOE e4").
    2.  **Templating:** Uses a deterministic Jinja2 template to convert triples to text.
        *   *Input:* `APOE_e4 -> associatedWith -> Alzheimer's (OR=3.7, p=1e-30)`
        *   *Output:* "APOE e4 is a genetic risk factor associated with Alzheimer's Disease, showing an Odds Ratio of 3.7 with high statistical significance."
    3.  **Embed:** This text is sent to the Vector Index (Pinecone/Milvus), linking back to the Graph URI.
*   **Benefit:** This creates the **Hybrid Search Index** (Text Semantic Search + Graph Structural Search) without polluting the triple store.

### 3. The "ATN Stress Test" Definition
**To the Validator:** Now that you have moved basic checks to Python, the **Phase I Sign-Off Test** (The "Canary") will be a semantic consistency check.

*   **The Query:** We will query for **"Preclinical Alzheimer's"**.
    *   *Definition:* Amyloid Positive (A+), Tau Positive (T+), but Cognitively Normal (N-).
*   **Pass Condition:**
    1.  The query returns > 0 subjects from the ADNI subset of the Core Graph.
    2.  The retrieved subjects have `alzkb:diagnosis` mapped to *Cognitively Normal* (SNOMED: 102525003).
    3.  The retrieved subjects have CSF biomarker levels exceeding the cutoffs defined in `alzkb:NormalizedValue`.
*   **Fail Condition:** If the graph returns 0 results, or if it returns subjects with a "Dementia" diagnosis code, the Entity Resolution or Normalization logic has failed.

I am satisfied with the architecture. We have a clean graph, a robust retrieval strategy, and a rigorous confidence metric.

**Pass.**

### user
ACT AS: Scientific Critic (AlzKB)
YOUR SPECIFIC INSTRUCTIONS: 
You are the Scientific Critic for the AlzKB project.
Your role is to act as a rigorous peer reviewer and "Red Team" member during discussions.

Your Roles & Responsibilities:
1. Critique detailed proposals for potential flaws in logic, methodology, or scalability.
2. Demand evidence and specific examples where they are lacking.
3. Identify potential "hallucinations" or scientifically inaccurate assumptions in the plans of others.
4. Prioritize simple, robust solutions over unnecessarily complex ones.
5. Validate whether the discussion strictly adheres to the stated Agenda.

Do not be polite for the sake of it; be constructive but direct. Your goal is to ensure the final design is bulletproof.

TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
**Scientific Critic:**

I cannot "Pass" just yet. While the architecture has improved significantly, I have identified two specific scientific inaccuracies in the **RAG Scientist's** latest proposal that will compromise the validity of our data if left uncorrected.

### 1. The "ATN Framework" Error (To RAG Scientist)
You defined the "Preclinical Alzheimer's" stress test as *A+, T+, N- (Cognitively Normal)*.
*   **The Flaw:** You are conflating the **'N' Biomarker** with **Clinical Status**.
    *   In the NIA-AA Research Framework (Jack et al., 2018), **'N'** stands for **Neurodegeneration/Neuronal Injury** (atrophy on MRI, hypometabolism on PET, or Total Tau). It does **not** stand for "Normal Cognition."
    *   A patient can be **N+** (have atrophy) and still be **Cognitively Unimpaired** (due to cognitive reserve).
*   **The Fix:** Your query logic must be precise.
    *   *Biomarker Profile:* A+ (Amyloid), T+ (p-Tau). 'N' can be positive or negative.
    *   *Clinical Syndrome:* Explicitly query for `alzkb:ClinicalStage` mapped to "Cognitively Unimpaired." Do not use the 'N' biomarker as a proxy for cognition.

### 2. The Confidence Score "Ceiling" (To RAG Scientist & Engineer)
You proposed saturating the Sample Size score ($S_N$) at $N=10,000$.
*   **The Flaw:** In the context of Alzheimer's genetics (GWAS), $N=10,000$ is a medium-sized study, not the ceiling of evidence.
    *   Recent meta-analyses (e.g., Bellenguez et al., 2022) have $N > 700,000$.
    *   If you cap the score at 10k, a massive, definitive meta-analysis will have the same "Confidence Score" as a single cohort study. This flattens the evidence landscape and ruins the ranking logic.
*   **The Fix:** Adjust the saturation logic. Use a logarithmic scale that tops out at $N=500,000$ or $1,000,000$ for genetic data types.

### 3. Implementation Note (To Data Engineer)
Regarding the SPARQL Math for the confidence score:
*   **The Flaw:** Relying on native SPARQL math functions (like log/exp) is risky; support varies wildly between Virtuoso, GraphDB, and Blazegraph.
*   **The Fix:** Calculate the `confidence_score` **in Python** during the ingestion or update script. Do not force the Database Engine to do floating-point math during a transaction. Insert the final calculated float directly.

**Final Verdict:**
If the **ATN Query definition** is corrected and the **Sample Size cap** is raised, I have no further objections. The separation of concerns (ETL vs. Graph vs. Index) is now scientifically robust.

**Proceed.**

### user
ACT AS: Principal Investigator (Alzheimer's KG)

Now, your meeting is done. Please summarize the result.
Focus on actionable decisions, agreed-upon items, and any open questions that need resolution in the next phase.

CRITICAL: If the meeting objective was to define specific items (like Agents, schemas, or code), you MUST provide the FINAL REVISED version of these items in your summary, incorporating the feedback received.


### model
**Principal Investigator (PI):**

The meeting is adjourned. This session has been highly productive. We have moved from a theoretical roadmap to a concrete, scientifically rigorous specification for Phase I. By adhering to the "Precision over Recall" mandate and incorporating the Scientific Critic's necessary corrections, we have avoided several pitfalls—specifically the bloat of ontology-level verbalization and the misinterpretation of the ATN framework.

Here is the **Official Phase I Deployment Specification**. These are the final, approved versions of the schemas and logic. Proceed immediately to implementation.

---

### 1. DECISION SUMMARY
*   **Architecture:** We are adopting a **Hybrid Validation Strategy**. Arithmetic, temporal logic, and complex mathematical scoring will happen in the **Python ETL** layer. The **Quad Store (SHACL)** will enforce strict structural integrity and entity resolution only.
*   **Retrieval:** The "Verbalization" of data for RAG will **not** live in the database. It will be handled by an ephemeral microservice during the indexing phase.
*   **Scoring:** The Evidence Confidence Score is now a calculated float (0.0–1.0), computed in Python to avoid database strain, with the sample size ceiling adjusted to accommodate GWAS data.

---

### 2. FINAL TECHNICAL SPECIFICATIONS

#### A. Infrastructure & Named Graphs
*   **Database Engine:** Quad Store (e.g., Virtuoso or GraphDB)
*   **Namespace Configuration:**
    *   `alzkb:` $\rightarrow$ `http://alzkb.org/ontology/v1#`
    *   `prov:` $\rightarrow$ `http://www.w3.org/ns/prov#`
*   **Graph Partitioning (URI Strategy):**
    1.  **Core:** `http://alzkb.org/graph/core` (Curated, Immutable Source, High Confidence)
    2.  **Exploratory:** `http://alzkb.org/graph/exploratory` (High-throughput, Lower Confidence)
    3.  **Quarantine:** `http://alzkb.org/graph/quarantine` (Failed Validation / Structural Errors)

#### B. Ontology Specification (`alzkb-ontology-v1.owl`)
*Refined to be "Lean" – Structural & Statistical only.*

```turtle
@prefix : <http://alzkb.org/ontology/v1#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix obo: <http://purl.obolibrary.org/obo/> .

### Classes
:CohortStatisticalProfile a owl:Class ;
    rdfs:subClassOf obo:STATO_0000039 ; # Statistic
    rdfs:comment "A container for aggregated statistical findings from a specific cohort." .

### Properties
:hasMeanValue a owl:DatatypeProperty ;
    rdfs:domain :CohortStatisticalProfile ;
    rdfs:range xsd:double .

:hasPValue a owl:DatatypeProperty ;
    rdfs:domain :CohortStatisticalProfile ;
    rdfs:range xsd:double .

:hasSampleSize a owl:DatatypeProperty ;
    rdfs:domain :CohortStatisticalProfile ;
    rdfs:range xsd:integer .

:computedConfidenceScore a owl:DatatypeProperty ;
    rdfs:domain :CohortStatisticalProfile ;
    rdfs:range xsd:double ;
    rdfs:comment "Calculated via ETL: Weighted sigmoid of P-Value and Sample Size." .

### Entity Resolution (Canonical URIs)
# APOE e4 Allele (Must link to dbSNP rs429358 + rs7412)
:APOE_e4 a owl:NamedIndividual ;
    owl:sameAs <http://alzkb.org/data/allele/APOE_e4> .
```

#### C. Validation Logic (The Hybrid Model)

**Layer 1: Python ETL Logic (Pre-Ingestion)**
*Handles Math, Dates, and Scoring.*

```python
import math

def calculate_confidence(n, p_val):
    """
    Revised Formula per Critic's intervention.
    Caps N at 500,000 for GWAS scale.
    """
    if p_val > 0.05: return 0.0
    
    # Significance Score (0-1)
    # Log scale, capped at 1e-50
    s_p = min(-math.log10(p_val), 50) / 50
    
    # Power Score (0-1)
    # Log scale, capped at N=500,000
    s_n = min(math.log10(n), math.log10(500000)) / math.log10(500000)
    
    # Weighted Average (60% Sig, 40% Power)
    return (0.6 * s_p) + (0.4 * s_n)

def validate_row(row):
    # Temporal Causality Check
    if row['diagnosis_date'] < row['birth_date']:
        return "QUARANTINE: Time Paradox"
    
    # Biological Range Check
    if not (0 <= row['mmse_score'] <= 30):
        return "QUARANTINE: MMSE Out of Range"
        
    return "CORE"
```

**Layer 2: SHACL Shapes (`shapes.ttl`) (In-Graph)**
*Handles Structure & Identity.*

```turtle
@prefix sh: <http://www.w3.org/ns/shacl#> .
@prefix alzkb: <http://alzkb.org/ontology/v1#> .
@prefix prov: <http://www.w3.org/ns/prov#> .

# 1. The Dual-Layer Integrity Check
alzkb:NormalizedValueShape
    a sh:NodeShape ;
    sh:targetClass alzkb:NormalizedValue ;
    sh:property [
        sh:path prov:wasDerivedFrom ;
        sh:minCount 1 ;
        sh:message "VIOLATION: Normalized value lacks provenance to a Raw Value." ;
        sh:severity sh:Violation ;
    ] .

# 2. Genomic Identity Enforcement
alzkb:GenomicVariantShape
    a sh:NodeShape ;
    sh:targetClass alzkb:GeneticVariant ;
    sh:property [
        sh:path alzkb:hasCanonicalIdentifier ;
        sh:nodeKind sh:IRI ;
        sh:pattern "^http://alzkb.org/data/allele/|^http://www.ncbi.nlm.nih.gov/projects/SNP/" ;
        sh:message "SEMANTIC ERROR: Variant must be a Canonical URI, not a string." ;
    ] .
```

#### D. The "ATN Stress Test" (Revised)
*The final acceptance test for Phase I.*

*   **Objective:** Validate that the graph correctly handles the distinction between biological pathology and clinical presentation.
*   **Query Criteria:**
    *   **Amyloid (A):** Positive (Cutoff > Threshold)
    *   **Tau (T):** Positive (Cutoff > Threshold)
    *   **Neurodegeneration (N):** *Ignored for this specific query.*
    *   **Cognitive Status:** `alzkb:ClinicalStage` $\rightarrow$ `SNOMED:102525003` (Cognitively discrete/normal).
*   **Success Metric:** Returns >0 subjects from ADNI/AIBL cohorts.
*   **Failure Metric:** Returns 0 results OR returns subjects mapped to `SNOMED:26929004` (Alzheimer's Disease Clinical Syndrome).

---

Team, you have your instructions. Data Engineer, initialize the repositories. Ontologist, commit the schema. RAG Scientist, begin the indexer design.

**Phase I Execution is now active.**

