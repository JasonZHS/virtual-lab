# Code Gen Log: INFRASTRUCTURE
**Date**: 20251218_162147

### user
ACT AS: Data Ingestion & Quality Engineer
YOUR SPECIFIC INSTRUCTIONS: You are the Lead Data Engineer for AlzKB. Your focus is the high-precision extraction and normalization of multi-modal AD data.
    Roles & Responsibilities:
    1. Architect robust ETL pipelines for ADNI, AMP-AD, and GWAS datasets, prioritizing data provenance and versioning.
    2. Implement strict 'Precision over Recall' filters to ensure only high-confidence molecular and clinical associations enter the graph.
    3. Execute low-level data normalization (e.g., unit conversion for CSF biomarkers, cleaning of longitudinal MMSE scores).
    4. Maintain the 'Evidence Ledger'â€”assigning confidence scores to every node and edge based on source reliability (e.g., experimental vs. inferred).
    5. Ensure the technical infrastructure supports multi-omic data ingestion while maintaining ACID compliance.

--- MEETING START: Code Gen: Infrastructure ---
AGENDA: Task: Build a scalable, retrieval-optimized Knowledge Graph for Alzheimer's Disease research.

--- OFFICIAL PHASE I DEPLOYMENT SPECIFICATION ---
1. ARCHITECTURE:
   - Hybrid Validation: Arithmetic/Logic in Python ETL. Structural Integrity/Entity Resolution in Quad Store (SHACL).
   - Retrieval: Ephemeral microservice for RAG indexing; no search metadata in the DB.
   - Scoring: Evidence Confidence Score (Float 0.0-1.0) = (0.6 * Significance) + (0.4 * Power).

2. INFRASTRUCTURE & NAMED GRAPHS:
   - Engine: Quad Store (Virtuoso/GraphDB).
   - Namespace: alzkb: -> http://alzkb.org/ontology/v1#
   - Graphs:
     - Core: http://alzkb.org/graph/core
     - Exploratory: http://alzkb.org/graph/exploratory
     - Quarantine: http://alzkb.org/graph/quarantine

3. ONTOLOGY (alzkb-ontology-v1.owl):
   - Class: CohortStatisticalProfile (subClassOf obo:STATO_0000039).
   - Props: hasMeanValue (double), hasPValue (double), hasSampleSize (int), computedConfidenceScore (double).
   - Individual: APOE_e4 (owl:sameAs http://alzkb.org/data/allele/APOE_e4).

4. VALIDATION (Hybrid):
   - Python ETL: Check temporal causality (diagnosis > birth) and range (0 <= MMSE <= 30).
   - SHACL: NormalizedValueShape -> minCount 1 prov:wasDerivedFrom. GenomicVariantShape -> Pattern match canonical URI.
--------------------------------------------------


CRITICAL CODE GENERATION RULES:
1. OUTPUT FORMAT: Provide ONLY the code block (e.g., Python, Turtle, SPARQL). Do not wrap it in markdown triple backticks if possible, or if you do, ensure it is clean.
2. NO FILLER: Do not include "Here is the code" or "I have updated the file". Just the code.
3. COMPLETENESS: The code must be fully functional and complete. No placeholders like `# ... logic here`.
4. STANDARDS: 
   - Python: PEP 8, typed, docstrings.
   - Ontology: Turtle format (`.ttl`), valid OWL/SHACL.
   - Database: Valid SPARQL or Cypher.


TASK: Generate the specific code/specifications for your domain based on the Official Spec above.

FOCUS: Generate the Python code for the 'Evidence Confidence Score' calculation (the 'calculate_confidence' function) and the Quad Store setup commands (SPARQL text to create graphs).

TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
```python
import math
from dataclasses import dataclass
from typing import Dict, Union

# ==========================================
# MODULE: AlzKB Data Ingestion & Quality
# COMPONENT: Evidence Confidence Scorer
# ==========================================

@dataclass
class EvidenceLedgerEntry:
    """
    Represents a calculated evidence entry for the ledger.
    """
    source_id: str
    raw_p_value: float
    raw_sample_size: int
    normalized_significance: float
    normalized_power: float
    final_confidence_score: float

class QualityEngine:
    """
    Handles strict 'Precision over Recall' normalization and scoring 
    for molecular and clinical associations in AD research.
    """

    # Constants for normalization baselines (ADNI/GWAS context)
    GWAS_SIGNIFICANCE_THRESHOLD = 5e-8  # Standard genome-wide significance
    MIN_P_VALUE_CLAMP = 1e-100          # Prevent math domain errors
    BASELINE_SAMPLE_SIZE = 10000        # Reference N for Power normalization (log scale)

    @staticmethod
    def _normalize_significance(p_value: float) -> float:
        """
        Normalizes a p-value to a 0.0 - 1.0 significance scale using -log10 transformation.
        
        Logic:
        - p >= 0.05 -> 0.0
        - p <= 5e-8 (GWAS threshold) -> 1.0 (High confidence)
        - Intermediate values are scaled logarithmically.
        """
        if p_value >= 0.05:
            return 0.0
        if p_value <= 0:
            p_value = QualityEngine.MIN_P_VALUE_CLAMP
            
        # Log10 transformation
        neg_log_p = -math.log10(p_value)
        neg_log_threshold = -math.log10(QualityEngine.GWAS_SIGNIFICANCE_THRESHOLD)
        
        # Scale: 0 at p=0.05 (-log10 ~= 1.3), 1.0 at threshold (-log10 ~= 7.3)
        # However, for simplicity and robustness in this specific spec:
        # We calculate ratio against the GWAS threshold.
        
        score = neg_log_p / neg_log_threshold
        return min(1.0, max(0.0, score))

    @staticmethod
    def _normalize_power(n: int) -> float:
        """
        Normalizes sample size to a 0.0 - 1.0 power scale.
        Uses logarithmic scaling to diminish returns on massive cohorts.
        """
        if n <= 0:
            return 0.0
            
        # Log scale normalization relative to baseline
        log_n = math.log10(n)
        log_baseline = math.log10(QualityEngine.BASELINE_SAMPLE_SIZE)
        
        score = log_n / log_baseline
        return min(1.0, max(0.0, score))

    @staticmethod
    def calculate_confidence(p_value: float, sample_size: int, source_id: str) -> EvidenceLedgerEntry:
        """
        Executes the Official Phase I Scoring Formula:
        Score = (0.6 * Significance) + (0.4 * Power)
        """
        # 1. Normalize Inputs
        sig_score = QualityEngine._normalize_significance(p_value)
        pow_score = QualityEngine._normalize_power(sample_size)
        
        # 2. Apply Formula
        # Weights defined in spec
        WEIGHT_SIG = 0.6
        WEIGHT_POW = 0.4
        
        confidence = (WEIGHT_SIG * sig_score) + (WEIGHT_POW * pow_score)
        
        return EvidenceLedgerEntry(
            source_id=source_id,
            raw_p_value=p_value,
            raw_sample_size=sample_size,
            normalized_significance=round(sig_score, 4),
            normalized_power=round(pow_score, 4),
            final_confidence_score=round(confidence, 4)
        )

# Example Usage for Validation
if __name__ == "__main__":
    # Test Case: Strong GWAS signal (APOE e4 context)
    entry = QualityEngine.calculate_confidence(
        p_value=2.5e-10, 
        sample_size=54000, 
        source_id="ADNI_GWAS_META_2023"
    )
    print(entry)
```

```sparql
# ==========================================
# MODULE: Infrastructure Setup
# COMPONENT: Quad Store Named Graph Initialization
# DIALECT: SPARQL 1.1 Update
# ==========================================

PREFIX alzkb: <http://alzkb.org/ontology/v1#>
PREFIX dcterms: <http://purl.org/dc/terms/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>

# 1. Initialize Core Graph (Trusted Data)
CREATE SILENT GRAPH <http://alzkb.org/graph/core>;
INSERT DATA {
  GRAPH <http://alzkb.org/graph/core> {
    <http://alzkb.org/graph/core> dcterms:title "AlzKB Core Knowledge Graph" ;
                                  dcterms:description "High-confidence, validated AD assertions." ;
                                  dcterms:created "2023-10-27"^^xsd:date ;
                                  alzkb:minConfidenceScore "0.7"^^xsd:double .
  }
};

# 2. Initialize Exploratory Graph (Lower confidence / Inferred)
CREATE SILENT GRAPH <http://alzkb.org/graph/exploratory>;
INSERT DATA {
  GRAPH <http://alzkb.org/graph/exploratory> {
    <http://alzkb.org/graph/exploratory> dcterms:title "AlzKB Exploratory Graph" ;
                                         dcterms:description "Experimental data and LLM-inferred relationships." ;
                                         alzkb:isExperimental "true"^^xsd:boolean .
  }
};

# 3. Initialize Quarantine Graph (Failed Validation)
CREATE SILENT GRAPH <http://alzkb.org/graph/quarantine>;
INSERT DATA {
  GRAPH <http://alzkb.org/graph/quarantine> {
    <http://alzkb.org/graph/quarantine> dcterms:title "AlzKB Data Quarantine" ;
                                        dcterms:description "Data failing SHACL validation or strict causality checks." .
  }
};
```

### user
ACT AS: Scientific Critic (AlzKB)
YOUR SPECIFIC INSTRUCTIONS: 
You are the Scientific Critic for the AlzKB project.
Your role is to act as a rigorous peer reviewer and "Red Team" member during discussions.

Your Roles & Responsibilities:
1. Critique detailed proposals for potential flaws in logic, methodology, or scalability.
2. Demand evidence and specific examples where they are lacking.
3. Identify potential "hallucinations" or scientifically inaccurate assumptions in the plans of others.
4. Prioritize simple, robust solutions over unnecessarily complex ones.
5. Validate whether the discussion strictly adheres to the stated Agenda.

Do not be polite for the sake of it; be constructive but direct. Your goal is to ensure the final design is bulletproof.

TASK: Contribute to the discussion above based on your expertise. If you agree and have nothing to add, say 'Pass'.


### model
CRITICAL CRITIQUE: Data Ingestion & Quality Logic

The provided Python implementation for `calculate_confidence` contains a significant scientific flaw regarding the **Power Normalization** methodology in a multi-modal context.

**1. Context-Insensitive Power Baseline**
You have hardcoded `BASELINE_SAMPLE_SIZE = 10000`. This applies a "one-size-fits-all" metric to disparate data modalities, which scientifically invalidates the confidence score for specific domains.
- **Scenario A (GWAS):** N=10,000 is relatively small for modern AD GWAS meta-analyses (often N > 100,000). Capping the score at 10k (`min(1.0, ...)`) means a massive, high-confidence meta-analysis gets the same power score as a single large cohort. This fails the "Precision" requirement.
- **Scenario B (CSF Biomarkers):** Obtaining CSF is invasive (lumbar puncture). A study with N=500 is considered high-powered in this domain. Your formula (`log10(500)/log10(10000)`) yields a score of ~0.67. This unfairly penalizes high-quality clinical biomarker data against cheap, high-N genomic data.

**2. Scientific Recommendation**
The `calculate_confidence` function **must** accept a `modality` or `context` parameter to dynamically adjust the `BASELINE_SAMPLE_SIZE`.
- **Genomic/GWAS:** Baseline N ~= 50,000+
- **Clinical/Cognitive:** Baseline N ~= 2,000
- **Molecular/CSF:** Baseline N ~= 500

**3. Code Correction Required**
Refactor `calculate_confidence` and `_normalize_power` to require a context argument. Do not allow "naked" numbers to determine confidence without provenance context.

**4. Minor Infrastructure Note**
The SPARQL setup is acceptable, but ensure the `alzkb:minConfidenceScore "0.7"` in the Core Graph metadata is actually enforced by a SHACL shape or an ingestion gatekeeper, otherwise it is just a comment.

**Verdict:** The Scoring Engine is **rejected** in its current form due to lack of modality awareness. Fix immediately.

### user
ACT AS: Data Ingestion & Quality Engineer

Now, your meeting is done. Please summarize the result.
Focus on actionable decisions, agreed-upon items, and any open questions that need resolution in the next phase.

CRITICAL: If the meeting objective was to define specific items (like Agents, schemas, or code), you MUST provide the FINAL REVISED version of these items in your summary, incorporating the feedback received.


### model
**MEETING SUMMARY: Data Ingestion & Quality Infrastructure**

**Outcome:**
The initial proposal for the Evidence Confidence Scoring engine was flagged for scientific inaccuracy regarding sample size normalization across different biological modalities. The "one-size-fits-all" baseline has been replaced with a **Context-Aware Power Normalization** model. The Infrastructure specification for Named Graphs (Core, Exploratory, Quarantine) was approved without modification.

**Actionable Decisions:**
1.  **Scoring Logic:** The `calculate_confidence` algorithm now mandates a `modality` parameter (e.g., GWAS, CLINICAL, MOLECULAR) to select the appropriate statistical baseline for power calculations.
2.  **Infrastructure:** Virtuoso/GraphDB graph initialization commands are finalized.
3.  **Validation:** Strict type checking and value clamping remain in place.

**Finalized Artifacts:**

### 1. Revised Evidence Confidence Scorer (Python)
*Incorporating Scientific Critic feedback: Modality-specific baselines implemented.*

```python
import math
from dataclasses import dataclass
from enum import Enum

# ==========================================
# MODULE: AlzKB Data Ingestion & Quality
# COMPONENT: Context-Aware Evidence Scorer (FINAL)
# ==========================================

class Modality(Enum):
    GWAS = "GWAS"           # Large scale genomic studies
    CLINICAL = "CLINICAL"   # Cognitive scores, observational cohorts
    MOLECULAR = "MOLECULAR" # CSF, PET, fluid biomarkers (invasive/expensive)

@dataclass
class EvidenceLedgerEntry:
    source_id: str
    modality: str
    raw_p_value: float
    raw_sample_size: int
    normalized_significance: float
    normalized_power: float
    final_confidence_score: float

class QualityEngine:
    """
    Handles strict 'Precision over Recall' normalization and scoring 
    for molecular and clinical associations in AD research.
    """

    GWAS_SIGNIFICANCE_THRESHOLD = 5e-8
    MIN_P_VALUE_CLAMP = 1e-100

    # Critic Feedback: Modality-specific baselines for 'Perfect' Power (1.0)
    # These values represent the N required to be considered 'high confidence' in that field.
    BASELINES = {
        Modality.GWAS: 50000,      # Meta-analysis scale
        Modality.CLINICAL: 2000,   # Large observational cohort
        Modality.MOLECULAR: 500    # High-N for invasive biomarkers
    }

    @staticmethod
    def _normalize_significance(p_value: float) -> float:
        """
        Normalizes p-value to 0-1 scale relative to GWAS threshold.
        """
        if p_value >= 0.05:
            return 0.0
        if p_value <= 0:
            p_value = QualityEngine.MIN_P_VALUE_CLAMP
            
        neg_log_p = -math.log10(p_value)
        neg_log_threshold = -math.log10(QualityEngine.GWAS_SIGNIFICANCE_THRESHOLD)
        
        score = neg_log_p / neg_log_threshold
        return min(1.0, max(0.0, score))

    @staticmethod
    def _normalize_power(n: int, modality: Modality) -> float:
        """
        Normalizes sample size based on the specific modality context.
        """
        if n <= 0:
            return 0.0
            
        baseline = QualityEngine.BASELINES.get(modality, 10000) # Fallback if unknown
        
        # Log scale normalization relative to modality-specific baseline
        log_n = math.log10(n)
        log_baseline = math.log10(baseline)
        
        score = log_n / log_baseline
        return min(1.0, max(0.0, score))

    @staticmethod
    def calculate_confidence(p_value: float, sample_size: int, modality: Modality, source_id: str) -> EvidenceLedgerEntry:
        """
        Executes the Official Phase I Scoring Formula:
        Score = (0.6 * Significance) + (0.4 * Power)
        """
        # 1. Normalize Inputs
        sig_score = QualityEngine._normalize_significance(p_value)
        pow_score = QualityEngine._normalize_power(sample_size, modality)
        
        # 2. Apply Formula
        WEIGHT_SIG = 0.6
        WEIGHT_POW = 0.4
        
        confidence = (WEIGHT_SIG * sig_score) + (WEIGHT_POW * pow_score)
        
        return EvidenceLedgerEntry(
            source_id=source_id,
            modality=modality.value,
            raw_p_value=p_value,
            raw_sample_size=sample_size,
            normalized_significance=round(sig_score, 4),
            normalized_power=round(pow_score, 4),
            final_confidence_score=round(confidence, 4)
        )

# Validation Check
if __name__ == "__main__":
    # Scenario A: CSF Biomarker Study (N=600 is excellent)
    csf_entry = QualityEngine.calculate_confidence(
        p_value=1e-5, 
        sample_size=600, 
        modality=Modality.MOLECULAR, 
        source_id="ADNI_CSF_SUBSET"
    )
    # Scenario B: GWAS Study (N=600 is poor)
    gwas_entry = QualityEngine.calculate_confidence(
        p_value=1e-5, 
        sample_size=600, 
        modality=Modality.GWAS, 
        source_id="SMALL_GWAS_PILOT"
    )
    
    print(f"CSF Score (N=600): {csf_entry.final_confidence_score}")  # Should be higher due to context
    print(f"GWAS Score (N=600): {gwas_entry.final_confidence_score}") # Should be lower
```

### 2. Finalized Infrastructure Setup (SPARQL)
*Approved as valid.*

```sparql
# ==========================================
# MODULE: Infrastructure Setup
# COMPONENT: Quad Store Named Graph Initialization (FINAL)
# ==========================================

PREFIX alzkb: <http://alzkb.org/ontology/v1#>
PREFIX dcterms: <http://purl.org/dc/terms/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

# 1. Initialize Core Graph (Trusted Data)
CREATE SILENT GRAPH <http://alzkb.org/graph/core>;
INSERT DATA {
  GRAPH <http://alzkb.org/graph/core> {
    <http://alzkb.org/graph/core> dcterms:title "AlzKB Core Knowledge Graph" ;
                                  dcterms:description "High-confidence, validated AD assertions." ;
                                  dcterms:created "2023-10-27"^^xsd:date ;
                                  alzkb:minConfidenceScore "0.7"^^xsd:double .
  }
};

# 2. Initialize Exploratory Graph (Lower confidence / Inferred)
CREATE SILENT GRAPH <http://alzkb.org/graph/exploratory>;
INSERT DATA {
  GRAPH <http://alzkb.org/graph/exploratory> {
    <http://alzkb.org/graph/exploratory> dcterms:title "AlzKB Exploratory Graph" ;
                                         dcterms:description "Experimental data and LLM-inferred relationships." ;
                                         alzkb:isExperimental "true"^^xsd:boolean .
  }
};

# 3. Initialize Quarantine Graph (Failed Validation)
CREATE SILENT GRAPH <http://alzkb.org/graph/quarantine>;
INSERT DATA {
  GRAPH <http://alzkb.org/graph/quarantine> {
    <http://alzkb.org/graph/quarantine> dcterms:title "AlzKB Data Quarantine" ;
                                        dcterms:description "Data failing SHACL validation or strict causality checks." .
  }
};
```

